<?xml version='1.0' encoding='utf-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chris Adams' Blog</title>
  <id>http://chris.improbable.org/</id>
  <link href="http://chris.improbable.org/feeds/all.atom" rel="self"/>
  <subtitle>Chris Adams’ personal weblog</subtitle>
  <author>
    <name>Chris Adams</name>
    <email>chris@improbable.org</email>
  </author>
  <updated>2013-10-04T04:00:00+00:00</updated>
  <entry>
    <title>Dear EFF: please don't pick the wrong fight</title>
    <id>http://chris.improbable.org/2013/10/4/dear-eff/</id>
    <link href="http://chris.improbable.org/2013/10/4/dear-eff/"/>
    <summary>The fight against DRM is not worth discarding your integrity. Misrepresenting the W3C's Encrypted Media Extensions will not do anything useful but it will hold the web back and make the EFF less effective.</summary>
    <content type="html">&lt;p&gt;
                        First, some background: I've been a supporter and donor to the &lt;a href="https://www.eff.org/"&gt;Electronic Frontier Foundation&lt;/a&gt; for a long time – at least 2001, although I believe I started earlier during the &lt;a href="http://wiki.openrightsgroup.org/wiki/Crypto_Wars"&gt;90s Crypto Wars&lt;/a&gt; – and opposed to to &lt;a href="http://en.wikipedia.org/wiki/Digital_rights_management"&gt;DRM&lt;/a&gt; for at least as long. I've also been a fan of &lt;a href="http://en.wikipedia.org/wiki/Danny_O%27Brien"&gt;Danny O'Brien&lt;/a&gt;'s reporting and personal blog for a similarly long time.
                    &lt;/p&gt;
                    &lt;p&gt;
                        Unfortunately, today had me reconsidering that support because of O'Brien's recent blog post: &lt;a href="https://www.eff.org/deeplinks/2013/10/lowering-your-standards"&gt;Lowering Your Standards: DRM and the Future of the W3C&lt;/a&gt; . I feel this marks a dangerous trend of playing very loose with the facts in an attempt to pressure the W3C to drop the &lt;a href="https://dvcs.w3.org/hg/html-media/raw-file/tip/encrypted-media/encrypted-media.html"&gt;Encrypted Media Extensions (EME)&lt;/a&gt; spec and that this is not only like to fail but actually backfire in ensuring that millions of people continue to access content through proprietary, closed systems.
                    &lt;/p&gt;
                    &lt;h3&gt;
                        Background
                    &lt;/h3&gt;
                    &lt;p&gt;
                        A little background information: most video played on the web and particularly commercial content uses &lt;a href="http://en.wikipedia.org/wiki/Adobe_Flash" title="Adobe Flash - Wikipedia, the free encyclopedia"&gt;Adobe's Flash&lt;/a&gt; or &lt;a href="http://en.wikipedia.org/wiki/Microsoft_Silverlight" title="Microsoft Silverlight - Wikipedia, the free encyclopedia"&gt;Microsoft's Silverlight&lt;/a&gt; plugins to run a video player inside a webpage. Both Flash and Silverlight are full programming environments with a significant range of capabilities beyond video playback and have significant overlap with the features provided by your browser. They're distributed as browser plugins, which require a hefty download to be installed before viewing anything, and both generally require proprietary tools for developers to create applications.
                    &lt;/p&gt;
                    &lt;p&gt;
                        They're annoying for developers because they require using a completely different set of technologies than you use for everything else on the web but many places will write that off as a cost of doing business. What's more of a concern is that both plugins have a history of security problems and neither Microsoft nor Adobe appear to be particularly motivated to build the kind of fast, reliable, automatic update system which the modern browsers have so in addition to requiring your users to download something before viewing content, you're contributing to one of the leading sources of security exploits for the average user. It also means that anyone who wishes to publish video on the web is generally subject to the development roadmap for one of two companies.
                    &lt;/p&gt;
                    &lt;p&gt;
                        HTML5 offers a way out of this mess: browsers could play back video directly, avoiding the massive external dependency and allowing them to make improvements for video as quickly as they do anything else rather than hoping a third-party developer wants to make improvements. &lt;a href="http://en.wikipedia.org/wiki/HTML5_video"&gt;HTML5 &amp;lt;video&amp;gt;&lt;/a&gt; is very easy to use, fast and has a consistent high-quality user experience. Unfortunately anyone looking to use it for commercial content will learn that the licensing rules from all of the major content owners require the use of DRM and thus HTML5 video is not an option.
                    &lt;/p&gt;
                    &lt;h4&gt;
                        What is EME, anyway?
                    &lt;/h4&gt;
                    &lt;p&gt;
                        The W3C's &lt;a href="https://dvcs.w3.org/hg/html-media/raw-file/tip/encrypted-media/encrypted-media.html"&gt;EME&lt;/a&gt; group is working on way to reduce this dependency by adding a general mechanism which allows the use of HTML5 video with a little bit of JavaScript to specify a &lt;abbr title="Content Decryption Module"&gt;CDM&lt;/abbr&gt; and a decryption key for the file. This allows content providers to use the entire modern web stack and limit the DRM dependency to a small chunk of code which handles only the actual decryption – dramatically lowering the &lt;a href="http://en.wikipedia.org/wiki/Attack_surface" title="Attack surface - Wikipedia, the free encyclopedia"&gt;attack surface&lt;/a&gt; and avoiding the need for anywhere near as frequent updates as the actual decryption mechanism is far less complex than the entire, largely-duplicative platform which Flash or Silverlight provide.
                    &lt;/p&gt;
                    &lt;h3&gt;
                        The problem
                    &lt;/h3&gt;
                    &lt;p&gt;
                        DRM does not work and all DRMed content has ended up being available in unencrypted form very quickly because the only way to make DRM work is by completely locking down a device to prevent its owner from running code which can access the unencrypted data and, of course, there's always the &lt;a href="http://en.wikipedia.org/wiki/Analog_hole" title="Analog hole - Wikipedia, the free encyclopedia"&gt;Analog Hole&lt;/a&gt;. The EFF has a &lt;a href="https://www.eff.org/issues/drm"&gt;long, laudable history attempting to educate the public and lawmakers&lt;/a&gt; about these issues and I completely support those efforts.
                    &lt;/p&gt;
                    &lt;p&gt;
                        Unfortunately, this effort has failed. No significant amount of commercial video on the web is available without DRM and users don't seem to care as the billions of dollars of sales through iTunes, Amazon, Google Play, etc. and Netflix is using somewhere around &lt;a href="http://arstechnica.com/tech-policy/2011/05/netflix-now-owns-almost-30-percent-of-north-american-fixed-internet-traffic/"&gt;30% of the total Internet traffic in North America&lt;/a&gt; to serve DRM-encumbered video, mostly using Silverlight. Clearly convenience and availability are more important to people.
                    &lt;/p&gt;
                    &lt;p&gt;
                        The EFF has been taking a hard-line position on EME, focused on slippery-slope claims:
                    &lt;/p&gt;
                    &lt;figure class="quote"&gt;&lt;blockquote&gt;
                            &lt;p&gt;
                                By approving this idea, the W3C has ceded control of the "user agent" (the term for a Web browser in W3C parlance) to a third-party, the content distributor. That breaks a—perhaps until now unspoken—assurance about who has the final say in your Web experience, and indeed who has ultimate control over your computing device.
                            &lt;/p&gt;
                        &lt;/blockquote&gt;…
                        &lt;blockquote&gt;
                            &lt;p&gt;
                                &lt;em&gt;A Web where you cannot cut and paste text; where your browser can't "Save As..." an image&lt;/em&gt;; where the "allowed" uses of saved files are monitored beyond the browser; where JavaScript is sealed away in opaque tombs; and maybe even where we can no longer effectively "View Source" on some sites, is a very different Web from the one we have today. It's a Web where user agents—browsers—must navigate a nest of enforced duties every time they visit a page. It's a place where the next Tim Berners-Lee or Mozilla, if they were building a new browser from scratch, couldn't just look up the details of all the "Web" technologies. They'd have to negotiate and sign compliance agreements with a raft of DRM providers just to be fully standards-compliant and interoperable.
                            &lt;/p&gt;
                        &lt;/blockquote&gt;
                        &lt;figcaption&gt;&lt;cite&gt;&lt;a href="https://www.eff.org/deeplinks/2013/10/lowering-your-standards"&gt;Lowering Your Standards: DRM and the Future of the W3C&lt;/a&gt;, Danny O'Brien, EFF&lt;/cite&gt; &lt;small&gt;(emphasis mine)&lt;/small&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;
                        This is similar to some of the past claims made by &lt;a href="http://craphound.com/bio.php" title="Cory Doctorow’s craphound.com"&gt;Cory Doctorow&lt;/a&gt;:
                    &lt;/p&gt;
                    &lt;figure class="quote"&gt;&lt;blockquote cite="http://www.theguardian.com/technology/blog/2013/mar/12/tim-berners-lee-drm-cory-doctorow"&gt;
                            &lt;p&gt;
                                The first of these conditions – "robustness" against end-user modification – is a blanket ban on all free/open source software (free/open source software, by definition, can be modified by its users). &lt;em&gt;That means that the two most popular browser technologies on the Web – WebKit (used in Chrome and Safari) and Gecko (used in Firefox and related browsers) – would be legally prohibited from implementing whatever "standard" the W3C emerges&lt;/em&gt;.
                            &lt;/p&gt;
                        &lt;/blockquote&gt;
                        &lt;figcaption&gt;&lt;cite&gt;&lt;a href="http://www.theguardian.com/technology/blog/2013/mar/12/tim-berners-lee-drm-cory-doctorow"&gt;What I wish Tim Berners-Lee understood about DRM&lt;/a&gt;&lt;/cite&gt;, Cory Doctorow, The Guardian. &lt;small&gt;(emphasis mine)&lt;/small&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;
                        Both of these are simply wrong: there is no meaningful distinction between what EME proposes and what is already the case with a browser plugin. If Firefox can play Flash or Silverlight content, it can decrypted video using a CDM which is either included in the host operating system, bundled under an agreement similar to Chrome's Flash plugin or installed by the user.
                    &lt;/p&gt;
                    &lt;p&gt;
                        The real problem is that they're arguing the wrong point: those requests have always been made and, in most cases, have already happened. The lack of a W3C standard hasn't prevented the Amazon Kindle app from preventing your ability to save unencrypted text, iTunes from blocking saving snippets of a rented movie, etc. and it hasn't prevented either Adobe or Microsoft from adding every DRM feature requested by the content owners. What this has done is ensured that the web community hasn't had much say in the process because all of the content is created and played using proprietary closed software.
                    &lt;/p&gt;
                    &lt;p&gt;
                        &lt;strong&gt;The EFF is shouting loudly but only Adobe and Microsoft will benefit&lt;/strong&gt;. There's no indication whatsoever that the studios are going to drop their DRM requirements if this W3C spec is scuttled – we'll just continue to see a lot of opaque plugin content and, of course, more pressure away from the web towards proprietary app stores. Mozilla's Asa Dotzler summed this up perfectly earlier today on &lt;a href="https://news.ycombinator.com/item?id=6492027"&gt;Hacker News:&lt;/a&gt; &lt;q cite="https://news.ycombinator.com/item?id=6492027"&gt;[T]he businesses (Hollywood) with the content that Web users want have done that math and decided that DRM through plug-ins and native apps is an EXCELLENT system and they're happy to keep mandating it forever. If Plug-ins go away, as they're slowly but surely doing, then native apps will be the only place to get this content.&lt;/q&gt;
                    &lt;/p&gt;
                    &lt;p&gt;
                        This approach also runs the risk of damaging the reputation of the EFF and making it less effective: beyond basic factual problems, exaggerating the risks will backfire badly if people look and – correctly – see that the situation isn't so terrible (Netflix at $10/month is absurdly popular despite the DRM) and discount future claims made by the EFF. They'll need that credibility as the &lt;a href="http://boingboing.net/2012/08/23/civilwar.html"&gt;war on general purpose computing&lt;/a&gt; continues — and Cory is not wrong to sound the alarm over that.
                    &lt;/p&gt;
                    &lt;p&gt;
                        What the open web community should be doing now is working to ensure that EME is designed in a way which improves security and reduces the proprietary footprint. If the standard for CDMs includes aggressive sandboxing it's a huge win for security alone even if all it does is turn Flash into a collective bad memory for web users. Additionally, separating the task of building a decryption module from building a high-performance video player with robust networking, makes it significantly easier for new vendors to enter the market and increases portability because so much less code needs to be adapted to a new platform.
                    &lt;/p&gt;
                    &lt;p&gt;
                        There are some interesting long-term trends, as well: more education about the risks of DRMed content is good and reducing what consumers are willing to pay for restricted content may be the best long-term strategy. Some of that effort needs to be directed towards content owners and providers who are thinking about investing in complex, expensive systems which don't actually work. A very interesting approach was highlighted by Mozilla's &lt;a href="https://brendaneich.com/"&gt;Brendan Eich&lt;/a&gt; earlier this year in the form of OTOY's pure-JavaScript video codec which in addition to avoiding all of the issues with binary plugins has first-class support for &lt;a href="http://en.wikipedia.org/wiki/Digital_watermarking" title="Digital watermarking - Wikipedia, the free encyclopedia"&gt;watermarking&lt;/a&gt;.
                    &lt;/p&gt;
                    &lt;figure class="quote"&gt;&lt;blockquote cite="https://brendaneich.com/2013/05/today-i-saw-the-future/"&gt;
                            &lt;p&gt;
                                Watermarking, not DRM. This could be huge. OTOY’s GPU cloud approach enables individually watermarking every intra-frame, and according to some of its Hollywood supporters including Ari Emanuel, this may be enough to eliminate the need for DRM.
                            &lt;/p&gt;
                        &lt;/blockquote&gt;
                        &lt;figcaption&gt;&lt;cite&gt;&lt;a href="https://brendaneich.com/2013/05/today-i-saw-the-future/"&gt;Today I saw the future&lt;/a&gt;, Brendan Eich (Mozilla CTO)&lt;/cite&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;
                        Obviously a shift away from the DRM obsession won't happen overnight but it's not impossible, either, as content owners are concerned about the market leverage which the major DRM vendors like Apple and Amazon have. There's space for smart players willing to back away from DRM in favor of an approach which works at least as well and doesn't require hardware vendors to sell out their users. As Brendan said, &lt;a href="https://news.ycombinator.com/item?id=6496128"&gt;&lt;q cite="https://news.ycombinator.com/item?id=6496128"&gt;there is hope&lt;/q&gt;&lt;/a&gt;.
                    &lt;/p&gt;
                    &lt;aside&gt;&lt;p&gt;
                            Disclaimer: while I work at the Library of Congress, I do not work for the Copyright Office and have no connection with official policy in any way and my job does not involve copyright, intellectual property or DRM in any way. Just to reiterate the site-wide footer message: this is solely my personal opinion written on my own time.
                        &lt;/p&gt;
                    &lt;/aside&gt;</content>
    <updated>2013-10-04T04:00:00+00:00</updated>
  </entry>
  <entry>
    <title>The NSA’s recklessness poses a risk to US business</title>
    <id>http://chris.improbable.org/2013/9/19/nsa-risks-to-us-businesses/</id>
    <link href="http://chris.improbable.org/2013/9/19/nsa-risks-to-us-businesses/"/>
    <summary>The NSA’s recklessness poses a risk to US business. IT is one of the bright spots in the US economy – perhaps our government should be more cautious about helping the competition…</summary>
    <content type="html">&lt;p&gt;
                        This is a great example of how the NSA's rogue actions are going to be endangering US IT companies for years: RSA has a security advisory out for several products, including a widely-used cryptography library, which defaulted to using the Dual EC DRBG random number generator, which we now know was released by the NSA with a backdoor to make it easier to spy on people.
                    &lt;/p&gt;
                    &lt;blockquote cite="http://www.wired.com/threatlevel/2013/09/rsa-advisory-nsa-algorithm/"&gt;
                        &lt;p&gt;
                            Amidst all of the confusion and concern over an encryption algorithm that may contain an NSA backdoor, RSA Security released an advisory to developer customers today noting that the algorithm is the default algorithm in one of its toolkits and strongly advises them to stop using the algorithm.
                        &lt;/p&gt;&lt;cite&gt;&lt;a href="http://www.wired.com/threatlevel/2013/09/rsa-advisory-nsa-algorithm/"&gt;RSA Tells Its Developer Customers: Stop Using NSA-Linked Algorithm&lt;/a&gt;, Kim Zetter, Wired&lt;/cite&gt;
                    &lt;/blockquote&gt;
                    &lt;p&gt;
                        This likely makes things weaker in a way which others could exploit – and given the high odds that people in e.g. China and Russia are racing to test that, it's likely that the NSA's actions exposed millions of people to unnecessary additional risk by weakening important software.
                    &lt;/p&gt;
                    &lt;p&gt;
                        It's likely even more damaging, however, to the US IT industry's future. We can ship updates to software relatively quickly but the question of trust is going to be much thornier: almost every RSA customer – and especially foreign ones – must be asking whether RSA was innocently dupe or actively collaborating. Given how much business they do with the US government, they're probably never going to be able to convincingly disprove that theory. Every other major security vendor in the US and certain allied countries is going to face a similar question: “How do we know you won't be in the news next?”
                    &lt;/p&gt;</content>
    <updated>2013-09-20T00:31:26.093447+00:00</updated>
  </entry>
  <entry>
    <title>Extracting images from scanned book pages</title>
    <id>http://chris.improbable.org/2013/8/31/extracting-images-from-scanned-pages/</id>
    <link href="http://chris.improbable.org/2013/8/31/extracting-images-from-scanned-pages/"/>
    <summary>A first step toward building a visual index for books automatically</summary>
    <content type="html">&lt;p&gt;
                        I work on a project which has placed &lt;a title="List of books available from the World Digital Library" href="http://www.wdl.org/en/search/?item_type=book"&gt;a number of books&lt;/a&gt; online. Over the years we've improved server performance and worked on a &lt;a href="https://github.com/LibraryOfCongress/wdl-viewer/"&gt;fast, responsive viewer for scanned books&lt;/a&gt; to make our books as accessible as possible but it's still challenging to help visitors find something of interest out of hundreds of thousands of scanned pages.
                    &lt;/p&gt;
                    &lt;p&gt;
                        &lt;a href="http://www.trevorowens.org"&gt;Trevor&lt;/a&gt; and I have discussed various ways to improve the situation and one idea which seemed promising was seeing how hard it would be to extract the images from digitized pages so we could present a visual index of an item. Trevor’s THATCamp CHNM post on &lt;a href="http://chnm2013.thatcamp.org/06/03/freeing-images-from-inside-digitized-books-and-newspapers/"&gt;Freeing Images from Inside Digitized Books and Newspapers&lt;/a&gt; got a favorable reception and since it kept coming up at work I decided to see how far I could get using &lt;a href="http://www.opencv.org/"&gt;OpenCV&lt;/a&gt;.
                    &lt;/p&gt;
                    &lt;p&gt;
                        Everything you see below is open-source and &lt;a href="mailto:chris@improbable.org"&gt;comments are highly welcome&lt;/a&gt;. I created a &lt;a href="https://github.com/acdha/image-mining/tree/book-illustration-detection"&gt;book-illustration-detection&lt;/a&gt; branch in my &lt;a href="https://github.com/acdha/image-mining/"&gt;image mining&lt;/a&gt; project (see my previous &lt;a href="http://chris.improbable.org/2013/06/30/reconstructing-thumbnails-using-opencv/"&gt;experiment reconstructing higher-resolution thumbnails from the masters&lt;/a&gt;) so feel free to fork it or open issues.
                    &lt;/p&gt;
                    &lt;p&gt;
                        The &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py"&gt;current process (locate-figures.py)&lt;/a&gt; is rather primitive:
                    &lt;/p&gt;
                    &lt;ol&gt;&lt;li&gt;
                            &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L30"&gt;convert the image to grayscale&lt;/a&gt;, which is both necessary for some of the algorithms
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L34"&gt;apply a binary filter converting image to black and white&lt;/a&gt;
                        &lt;/li&gt;
                        &lt;li&gt;Optionally, apply an &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L61-L67"&gt;erode&lt;/a&gt; or &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L70-L76"&gt;dilate&lt;/a&gt; filter (see the &lt;a href="http://docs.opencv.org/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html"&gt;OpenCV erosion and dilation tutorial&lt;/a&gt;)
                        &lt;/li&gt;
                        &lt;li&gt;Optionally, apply &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L80"&gt;Canny edge detection&lt;/a&gt; (&lt;a href="http://docs.opencv.org/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html"&gt;OpenCV tutorial&lt;/a&gt;)
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L83"&gt;find contours&lt;/a&gt; (i.e. what appear to be lines) (&lt;a href="http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/find_contours/find_contours.html"&gt;OpenCV tutorial&lt;/a&gt;)
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;a href="https://github.com/acdha/image-mining/blob/914b5c2c2d83508826933ca0b5b92ab28c6f97b8/bin/locate-figures.py#L95-L122"&gt;Filter contours which are very small or very large&lt;/a&gt;, to avoid extracting small things like defects, letters, etc. or large artifacts like borders from the scanning process which span an entire edge
                        &lt;/li&gt;
                    &lt;/ol&gt;&lt;p&gt;
                        The program requires Python, &lt;a href="http://www.opencv.org"&gt;OpenCV&lt;/a&gt; and &lt;a href="http://numpy.org/"&gt;numpy&lt;/a&gt;, all of which should be easy to install on Ubuntu/Debian Linux or using &lt;a href="http://brew.sh/"&gt;Homebrew&lt;/a&gt; on OS X. When the prerequisites are installed, the program can be run like this:
                    &lt;/p&gt;
                    &lt;figure&gt;&lt;img style="max-width: 60%" src="applying-filters-2.jpg"&gt;&lt;figcaption&gt;
                            Applying filters interactively, with contours and their bounding boxes displayed
                            &lt;pre&gt;
&lt;code&gt;locate-figures.py --interactive 211_1_82.png sn99021999_1913-08-31_1_1.png&lt;/code&gt;
&lt;/pre&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;
                        Results
                    &lt;/h3&gt;
                    &lt;p&gt;
                        The results are quite promising:
                    &lt;/p&gt;&lt;!-- TODO: finish migrating to sane CSS for figures --&gt;
                    &lt;div style="text-align: center"&gt;
                        &lt;figure&gt;&lt;a href="http://chroniclingamerica.loc.gov/lccn/sn99021999/1913-08-31/ed-1/seq-1/"&gt;&lt;img style="border: solid blue 1px" width="291" height="309" src="/experiments/opencv/page-image-extraction/sn99021999_1913-08-31_1_1%20extract%205489.jpg"&gt;&lt;/a&gt;
                            &lt;figcaption&gt;
                                Extracted cartoon from the &lt;a href="http://chroniclingamerica.loc.gov/lccn/sn99021999/1913-08-31/ed-1/seq-1/"&gt;Omaha Daily Bee front page for August 31st, 1913&lt;/a&gt; &lt;a href="http://chroniclingamerica.loc.gov/lccn/sn99021999/1913-08-31/ed-1/seq-1.jp2"&gt;JPEG-2000 Master&lt;/a&gt; (courtesy of &lt;a href="http://chroniclingamerica.loc.gov/"&gt;Chronicling America&lt;/a&gt;)
                            &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;a href="http://www.wdl.org/en/item/211/view/1/77/"&gt;&lt;img style="border: solid blue 1px" src="/experiments/opencv/page-image-extraction/211/211_1_77%20extract%201622.jpg"&gt;&lt;/a&gt;
                            &lt;figcaption&gt;
                                Extracted illustration from &lt;a href="http://www.wdl.org/en/item/211/"&gt;The Amazon and Madeira Rivers: Sketches and Descriptions from the Note-Book of an Explorer&lt;/a&gt;
                            &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;a href="http://www.wdl.org/en/item/211/view/1/246/"&gt;&lt;img style="border: solid blue 1px" src="/experiments/opencv/page-image-extraction/211/211_1_246%20extract%201654.jpg"&gt;&lt;/a&gt;
                            &lt;figcaption&gt;
                                Extracted illustration from &lt;a href="http://www.wdl.org/en/item/211/"&gt;The Amazon and Madeira Rivers: Sketches and Descriptions from the Note-Book of an Explorer&lt;/a&gt;
                            &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;a href="http://www.wdl.org/en/item/101/view/1/26/"&gt;&lt;img style="border: solid blue 1px" src="/experiments/opencv/page-image-extraction/101/101_1_26%20extract%201267.jpg"&gt;&lt;/a&gt;
                            &lt;figcaption&gt;
                                Extracted print from &lt;a href="http://www.wdl.org/en/item/101/"&gt;Guide to the Great Siberian Railway&lt;/a&gt;
                            &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;
                    &lt;p&gt;
                        There are, of course, some problems:
                    &lt;/p&gt;
                    &lt;figure style="text-align: center"&gt;&lt;a href="http://www.wdl.org/en/item/211/view/1/253/"&gt;&lt;img style="border: solid blue 1px" src="/experiments/opencv/page-image-extraction/211/211_1_253%20extract%20805.jpg"&gt;&lt;/a&gt; &lt;a href="http://www.wdl.org/en/item/211/view/1/253/"&gt;&lt;img style="border: solid blue 1px" src="/experiments/opencv/page-image-extraction/211/211_1_253%20extract%2010415.jpg"&gt;&lt;/a&gt;
                        &lt;figcaption&gt;
                            Multiple contours were detected in multiple points of this illustration but unfortunately they weren't seen as contiguous and both were large enough to be extracted
                            &lt;aside&gt;
                                (Source: &lt;a href="http://www.wdl.org/en/item/211/"&gt;The Amazon and Madeira Rivers: Sketches and Descriptions from the Note-Book of an Explorer&lt;/a&gt;)
                            &lt;/aside&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;
                        The full results are worth reviewing – I was surprised at the quality from the initial pass:
                    &lt;/p&gt;
                    &lt;ul&gt;&lt;li&gt;
                            &lt;a href="/experiments/opencv/page-image-extraction/211/"&gt;The Amazon and Madeira Rivers: Sketches and Descriptions from the Note-Book of an Explorer&lt;/a&gt;
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;a href="/experiments/opencv/page-image-extraction/101/"&gt;Guide to the Great Siberian Railway&lt;/a&gt;
                        &lt;/li&gt;
                    &lt;/ul&gt;&lt;p&gt;
                        There are some obvious areas for improvement such as attempting to prevent the above problem by filtering boxes which are entirely contained within other boxes. It would also be interesting to attempt to examine the surrounding area to see whether there appears to be a caption.
                    &lt;/p&gt;
                    &lt;p&gt;
                        Cool ideas? Deep experience with image processing? &lt;a href="mailto:chris@improbable.org"&gt;I'd love to hear what you think&lt;/a&gt;.
                    &lt;/p&gt;</content>
    <updated>2013-08-31T22:15:00+00:00</updated>
  </entry>
  <entry>
    <title>How simple web kiosks have become thanks to Webconverger</title>
    <id>http://chris.improbable.org/2013/8/29/how-simple-web-kiosks-have-become/</id>
    <link href="http://chris.improbable.org/2013/8/29/how-simple-web-kiosks-have-become/"/>
    <summary>Creating a fullscreen web app kiosk has never been easier thanks to Webconverger</summary>
    <content type="html">&lt;p&gt;
                        Recently I decided it was time to get around to creating a dashboard for the &lt;a href="http://www.wdl.org/"&gt;World Digital Library&lt;/a&gt;. We'd talked about it for years but there was always the hassle of maintaining another system acting as a deterrent for a side project. Since browsers have become more capable, however, I decided yesterday to see how far I could get with a pure static HTML page and a static boot-to-browser system.
                    &lt;/p&gt;
                    &lt;p&gt;
                        I quickly found &lt;a href="http://webconverger.org/"&gt;Webconverger&lt;/a&gt;, which is a Debian derivative intended for kiosks, schools &amp;amp; libraries, etc. A quick look at the &lt;a href="http://webconverger.org/API/"&gt;API documentation&lt;/a&gt; showed that all of the configuration I needed was possible simply by editing the boot-loader config. I tossed together a quick HTML page which loaded various things from Twitter, &lt;a href="https://graphite.readthedocs.org"&gt;Graphite&lt;/a&gt;, Atom feeds, etc. and tossed it into an S3 bucket.
                    &lt;/p&gt;
                    &lt;h2&gt;
                        Building the bootable USB key
                    &lt;/h2&gt;
                    &lt;ol&gt;&lt;li&gt;I downloaded the &lt;a href="http://dl.webconverger.com/latest.iso"&gt;ISO image&lt;/a&gt; and used dd to transfer it to a spare USB key (thanks conference vendors!) and booted to confirm that my hardware was supported and that the page looked great after typing the URL into the browser.
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;p&gt;
                                As per the documentation, everything worked automatically once I added a few boot parameters at startup: &lt;code&gt;noblank chrome=webcfullscreen homepage=http://example.org&lt;/code&gt;
                            &lt;/p&gt;
                            &lt;p&gt;
                                Unfortunately, on OS X there's no easy way to mount an ISO image read-write so it's not as simple as editing the file in your favorite editor…
                            &lt;/p&gt;
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;p&gt;
                                Now for the applied laziness: there's a paid configuration service and full developer instructions, and for that matter there are generic ISO editing tools, but I really just wanted to edit a text file which lives in &lt;code&gt;/boot/live.cfg&lt;/code&gt; and it felt somewhat excessive to setup a full toolchain to change a few bytes of ASCII text
                            &lt;/p&gt;
                            &lt;p&gt;
                                Solution: make a copy of the ISO file, open up &lt;a href="http://ridiculousfish.com/hexfiend/"&gt;Hex Fiend&lt;/a&gt; and search for a string which would only appear in the target file, as determined by looking in &lt;code&gt;/boot/live.cfg&lt;/code&gt; after mounting the ISO read-only:&lt;br&gt;&lt;code&gt;append initrd=/live/initrd2.img boot=live skipconfig quiet splash&lt;/code&gt;
                            &lt;/p&gt;
                            &lt;aside&gt;
                                (Did I really just describe hex-editing a binary file as the easy option? Well, yes — it's a standard system search dialog, a click to move the cursor, pasting, a click to change panes and holding down the 0 key for a few seconds. If you've ever tried to get Word to format a large document consistently, this is a piece of cake…)
                            &lt;/aside&gt;&lt;/li&gt;
                        &lt;li&gt;It's not quite simple as simply inserting text because I don't want to corrupt the ISO image by changing the size but fortunately there's a ton of boilerplate which I don't need for a simple dashboard. Like most C programs, the bootloader is almost certain to stop reading once it finds null bytes and indeed a quick glance shows a block of 00 bytes starting after the final character in the file.
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;p&gt;
                                A quick pass through an editor and I have the following trimmed down config with the customizations on the end of the append line:
                            &lt;/p&gt;
                            &lt;pre style="white-space: pre; word-wrap: normal; overflow-x: scroll"&gt;
DEFAULT live-686-pae

label live-686-pae
        menu label 686-pae: Live
        kernel /live/vmlinuz2
        append initrd=/live/initrd2.img boot=live skipconfig quiet splash bootfrom=removable &lt;strong&gt;noblank chrome=webcfullscreen homepage=http://example.org&lt;/strong&gt;
&lt;/pre&gt;
                            &lt;p&gt;
                                Past it in at the start of live.cfg, switch to the hex pane and pad out the file with NULLs until the original text is completely replaced.
                            &lt;/p&gt;
                        &lt;/li&gt;
                        &lt;li&gt;Use dd to write the modified ISO to your USB key and reboot
                        &lt;/li&gt;
                    &lt;/ol&gt;&lt;figure&gt;&lt;img src="dashboard.jpg" width="1360" height="768"&gt;&lt;figcaption&gt;
                            Screenshot of the final dashboard
                        &lt;/figcaption&gt;&lt;/figure&gt;</content>
    <updated>2013-08-29T08:42:00+00:00</updated>
  </entry>
  <entry>
    <title>Reconstructing thumbnails using OpenCV</title>
    <id>http://chris.improbable.org/2013/6/30/reconstructing-thumbnails-using-opencv/</id>
    <link href="http://chris.improbable.org/2013/6/30/reconstructing-thumbnails-using-opencv/"/>
    <summary>Building higher-resolution thumbnails given the original master file and a lot of CPU time</summary>
    <content type="html">&lt;h3&gt;
                        Background
                    &lt;/h3&gt;
                    &lt;p&gt;
                        I work with collections of scanned images which have been on the web for awhile. Each one has a hand-selected thumbnail as well as full-image files at various resolutions. Over the years, our default image display size has increased and we're approaching the point where our original thumbnail or reference image size is unpleasantly small. Unfortunately we don't have the data for the various image processing steps which were used to generate these thousands of thumbnails.
                    &lt;/p&gt;
                    &lt;p&gt;
                        I recently decided to experiment with &lt;a href="http://www.opencv.org"&gt;OpenCV&lt;/a&gt; to see whether it would be reasonable to locate the original location within the source files so we can extract new versions at arbitrary resolutions.
                    &lt;/p&gt;
                    &lt;h3&gt;
                        Techniques
                    &lt;/h3&gt;
                    &lt;p&gt;
                        At first glance, the &lt;a href="http://docs.opencv.org/doc/tutorials/imgproc/histograms/template_matching/template_matching.html"&gt;OpenCV template matching tutorial&lt;/a&gt; appears to be perfect for the job: give it a source image and a template image and it will attempt to locate the latter in the former. Unfortunately, this falls apart when the template image has been scaled or rotated, which is the often the case for our collections: all the template matching code is doing is sliding the template image around the source image one pixel at a time and measuring the difference!
                    &lt;/p&gt;
                    &lt;p&gt;
                        Fortunately, there are far more advanced techniques in the modern arsenal for building what are known as scale and rotation invariant feature descriptors. Unless you're a CS grad student, you'll want to start with OpenCV's extensive &lt;a href="http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)"&gt;Feature Detection&lt;/a&gt; suite. This example shows an even harder variant of our problem: &lt;a href="http://docs.opencv.org/doc/tutorials/features2d/feature_homography/feature_homography.html#feature-homography"&gt;using feature homography to locate a significantly distorted image in a photograph&lt;/a&gt; .
                    &lt;/p&gt;
                    &lt;p&gt;
                        For a first pass I'm using &lt;a href="http://en.wikipedia.org/wiki/SURF"&gt;&lt;abbr title="Speeded Up Robust Features"&gt;SURF&lt;/abbr&gt;&lt;/a&gt; with the brute-force matcher, although that will likely change once I take time to compare the other options. This has yielded great results, although the actual match calculation could be faster.
                    &lt;/p&gt;
                    &lt;h3&gt;
                        The Results
                    &lt;/h3&gt;
                    &lt;p&gt;
                        &lt;a href="https://github.com/acdha/image-mining/blob/master/bin/locate-thumbnail.py"&gt;locate-thumbnail.py&lt;/a&gt; has a pretty simple interface which allows you to supply a list of file pairs. It will attempt to reconstruct the original thumbnail from the master image and save the result in the same location. Since the assumption is that you are starting with thumbnails which were originally derived from the master image, it will automatically skip an image if an insufficient number of matching points are detected and since my goal was to get consistently sized output thumbnails the output is coerced to be perfectly rectangular, although it will be rotated by multiples of 90º to match the original orientation as you can see in several examples below.
                    &lt;/p&gt;
                    &lt;p&gt;
                        A basic session looks like this:
                    &lt;/p&gt;
                    &lt;pre class="prettyprint lang-sh"&gt;
$ for i in `seq 1 10`; do echo $i; curl -sko $i.jpg http://content.wdl.org/$i/thumbnail/308x255.jpg -O http://content.wdl.org/$i.png;  done
1
2
…
$ locate-thumbnail.py --display --save-visualization {1..10}.{jpg,png}
INFO locate_thumbnail: Attempting to locate 1.jpg within 1.png
INFO locate_thumbnail: Found 147 matches
INFO find_homography: 130 inliers, 147 matched features
INFO reconstruct_thumbnail: Reconstructing thumbnail from source image
INFO reconstruct_thumbnail: Thumbnail bounds within source image: [[190, 407], [870, 407], [868, 969], [191, 970]]
INFO reconstruct_thumbnail: Resizing from (563, 680) to (308, 255)
Master dimensions: 1271x1024
Thumbnail dimensions: 255x308
Reconstructed thumb dimensions: 255x308
INFO locate_thumbnail: Saved reconstructed thumbnail 1.reconstructed.jpg
INFO locate_thumbnail: Saved match visualization 1.visualized.jpg
INFO locate_thumbnail: Attempting to locate 2.jpg within 2.png
&lt;/pre&gt;
                    &lt;h4&gt;
                        Examples
                    &lt;/h4&gt;
                    &lt;figure&gt;&lt;img src="http://content.wdl.org/10/thumbnail/308x255.jpg"&gt;&lt;img src="examples/10.reconstructed.jpg"&gt;&lt;br&gt;&lt;a href="examples/10.visualized.jpg"&gt;&lt;img src="examples/10.visualized.jpg"&gt;&lt;/a&gt;
                        &lt;figcaption&gt;
                            Reconstructed thumbnail using the &lt;a href="http://www.wdl.org/media/10.png"&gt;PNG&lt;/a&gt; for &lt;a href="http://www.wdl.org/10"&gt;An Actor in the Role of Sato Norikiyo who Becomes Saigyo: An Actor in the Role of Yoshinaka&lt;/a&gt; from the &lt;a href="http://www.wdl.org"&gt;World Digital Library&lt;/a&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img src="http://content.wdl.org/3/thumbnail/308x255.jpg"&gt;&lt;img src="examples/3.reconstructed.jpg"&gt;&lt;br&gt;&lt;a href="examples/3.visualized.jpg"&gt;&lt;img src="examples/3.visualized.jpg"&gt;&lt;/a&gt;
                        &lt;figcaption&gt;
                            Reconstructed thumbnail using the &lt;a href="http://www.wdl.org/media/3.png"&gt;PNG&lt;/a&gt; for &lt;a href="http://www.wdl.org/3"&gt;Maps of Ezo, Sakhalin, and Kuril Islands&lt;/a&gt; from the &lt;a href="http://www.wdl.org"&gt;World Digital Library&lt;/a&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img src="http://content.wdl.org/1/thumbnail/308x255.jpg"&gt;&lt;img src="examples/1.reconstructed.jpg"&gt;&lt;br&gt;&lt;a href="examples/1.visualized.jpg"&gt;&lt;img src="examples/1.visualized.jpg"&gt;&lt;/a&gt;
                        &lt;figcaption&gt;
                            Reconstructed thumbnail using the &lt;a href="http://www.wdl.org/media/1.png"&gt;PNG&lt;/a&gt; for &lt;a href="http://www.wdl.org/1"&gt;Antietam, Maryland. Allan Pinkerton, President Lincoln, and Major General John A. McClernand: Another View&lt;/a&gt; from the &lt;a href="http://www.wdl.org"&gt;World Digital Library&lt;/a&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img src="http://content.wdl.org/22/thumbnail/308x255.jpg"&gt;&lt;img src="examples/22.reconstructed.jpg"&gt;&lt;br&gt;&lt;a href="examples/22.visualized.jpg"&gt;&lt;img src="examples/22.visualized.jpg"&gt;&lt;/a&gt;
                        &lt;figcaption&gt;
                            Reconstructed thumbnail using the &lt;a href="http://www.wdl.org/media/22.png"&gt;PNG&lt;/a&gt; for &lt;a href="http://www.wdl.org/22"&gt;The Island and City of Metropolitan Goa of India&lt;/a&gt; from the &lt;a href="http://www.wdl.org"&gt;World Digital Library&lt;/a&gt;
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;
                        Quite Possibly Working Code
                    &lt;/h4&gt;
                    &lt;p&gt;
                        All of the code above is available on Github and should run anywhere OpenCV is available: &lt;a href="https://github.com/acdha/image-mining"&gt;https://github.com/acdha/image-mining&lt;/a&gt;. To the extent allowed by law, it has been released into the public domain.
                    &lt;/p&gt;</content>
    <updated>2013-06-30T20:25:23.108473+00:00</updated>
  </entry>
  <entry>
    <title>Reconsidering modernizr.js performance defaults</title>
    <id>http://chris.improbable.org/2013/4/17/reconsidering-modernizr-defaults/</id>
    <link href="http://chris.improbable.org/2013/4/17/reconsidering-modernizr-defaults/"/>
    <summary>HTML5 won, let's reconsider the defaults for a post-IE8 world</summary>
    <content type="html">&lt;p&gt;
                        &lt;a href="http://modernizr.com"&gt;Modernizr&lt;/a&gt; is an incredibly useful tool for detecting browser capabilities and dealing with old browsers — it played a key role in the explosion of HTML5 feature adoption and quite rightly shows up all over the web.
                    &lt;/p&gt;
                    &lt;p&gt;
                        However, it's 2013 and web performance is critical for many sites but unfortunately the place where modernizr.js shows up on most sites is in the &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt;, violating one of the most important rules of web performance: &lt;a href="http://developer.yahoo.com/blogs/ydn/high-performance-sites-rule-6-move-scripts-bottom-7200.html"&gt;move scripts to the bottom of the page&lt;/a&gt; . This is unsurprising as the &lt;a href="http://modernizr.com/docs/#installing"&gt;installation instructions&lt;/a&gt; suggest exactly this:
                    &lt;/p&gt;
                    &lt;blockquote cite="http://modernizr.com/docs/#installing"&gt;
                        Drop the script tags in the &amp;lt;head&amp;gt; of your HTML. For best performance, you should have them follow after your stylesheet references. The reason we recommend placing Modernizr in the head is two-fold: the HTML5 Shiv (that enables HTML5 elements in IE) must execute before the &amp;lt;body&amp;gt;, and if you’re using any of the CSS classes that Modernizr adds, you’ll want to prevent a FOUC.&lt;br&gt;
                        If you don't support IE8 and don't need to worry about FOUC, feel free to include modernizr.js whereever.
                    &lt;/blockquote&gt;
                    &lt;p&gt;
                        That last part is the key: the only thing which Modernizr *needs* to run in the head is the &lt;a href="https://code.google.com/p/html5shiv/"&gt;html5shiv&lt;/a&gt;, which is only needed for Internet Explorer 8 and earlier — less than 10% of my main project's traffic.
                    &lt;/p&gt;
                    &lt;p&gt;
                        The performance impact will vary depending on your site, how well optimized everything else is and how well connected your users are but it's important to remember that a script in the &lt;code&gt;&amp;lt;head&amp;gt;&lt;/code&gt; will block rendering &lt;strong&gt;and&lt;/strong&gt; further script execution until it completes so you're looking at at least one full server round-trip, including the time to download ~5KB of Modernizr code, before the browser can continue. Modern browsers often scan ahead looking for additional resources to start transferring but in some quick &lt;a href="http://webpagetest.org"&gt;webpagetest.org&lt;/a&gt; runs I found this was adding a solid 100ms (or ~15%) to the time IE9 took my page to start rendering for a well-connected machine in a major datacenter — for those of us with a global audience, the impact is likely to be much worse unless you have great CDN coverage and high enough traffic to keep the caches warm everywhere.
                    &lt;/p&gt;
                    &lt;p&gt;
                        Since the html5shiv is only needed for IE8, reclaiming that extra speed for everyone else seems like an easy win – and one which avoids making the web slower in order to subsidize people who haven't upgraded yet. Here's what it looks like:
                    &lt;/p&gt;
                    &lt;pre&gt;
&lt;code class="html"&gt;&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;
        …
        &amp;lt;!--[if lt IE 9]&amp;gt;
            &amp;lt;script src="html5shiv.min.js"&amp;gt;&amp;lt;/script&amp;gt;
        &amp;lt;![endif]--&amp;gt;
    &amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
        …
        &amp;lt;script src="modernizr.custom.min.js"&amp;gt;&amp;lt;/script&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
                    &lt;p&gt;
                        As IE8 continues to fade into the sunset, now might be a good time to start moving old shims and polyfills into the slow-lane so we're optimizing for the future, not the past.
                    &lt;/p&gt;</content>
    <updated>2013-04-17T19:00:56+00:00</updated>
  </entry>
  <entry>
    <title>On PDF preservation risks</title>
    <id>http://chris.improbable.org/2012/7/27/pdf-preservation-risks/</id>
    <link href="http://chris.improbable.org/2012/7/27/pdf-preservation-risks/"/>
    <summary>On the risks facing long-term preservation of PDF files</summary>
    <content type="html">&lt;aside&gt;&lt;i&gt;(In a staggering, entirely–appropriate display of irony, this comment was rescued from the now defunct &lt;a href="http://libraries.stackexchange.com"&gt;Libraries &amp;amp; Information Science StackExchange&lt;/a&gt;: &lt;a href="http://libraries.stackexchange.com/questions/964/what-preservation-risks-are-associated-with-the-pdf-file-format"&gt;What preservation risks are associated with the PDF file format?&lt;/a&gt; — Chris)&lt;/i&gt;
                    &lt;/aside&gt;&lt;blockquote cite="%20http://libraries.stackexchange.com/questions/964/what-preservation-risks-are-associated-with-the-pdf-file-format"&gt;
                        &lt;p&gt;
                            This started out as a comment on @dsalo's excellent answer above but rather quickly expanded beyond 500 characters:
                        &lt;/p&gt;
                        &lt;p&gt;
                            &lt;a href="http://en.wikipedia.org/wiki/PDF"&gt;PDF&lt;/a&gt; is a &lt;a href="http://en.wikipedia.org/wiki/Container_format_%28digital%29"&gt;container format&lt;/a&gt;: a single PDF file has metadata and one or more content streams, conceptually similar to a ZIP archive containing multiple files. The core PDF format is based on a subset of &lt;a href="http://en.wikipedia.org/wiki/PostScript"&gt;PostScript&lt;/a&gt;, which is a programming language designed to produce graphics, and common graphics formats, but over time the format was expanded to allow streams to contain any type of data.
                        &lt;/p&gt;
                        &lt;ol&gt;&lt;li&gt;
                                &lt;p&gt;
                                    The PDF format is very complicated and pulls in several other complex specifications. In practice, the vast majority of PDF files were only validated by testing whether Adobe Acrobat can display them as intended and it is quite common to have PDF encoders generate output which breaks the standard in ways which Acrobat tolerates, leaving the problem to be detected only when the file is first used with other tools.
                                &lt;/p&gt;
                            &lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    While the subset of PostScript supported in PDF is not as capable as full PostScript (fortunately, as the latter which is &lt;a href="http://en.wikipedia.org/wiki/Turing_completeness"&gt;Turing Complete&lt;/a&gt;), it is still the case that what you actually have is executable program code and thus the only way to display PDF content is to execute each PostScript command in order:
                                &lt;/p&gt;
                                &lt;pre&gt;
&lt;code&gt;/Times findfont 100 scalefont setfont
                        10 10 moveto
                        .5 .5 .5 setrgbcolor
                        (Hello World) true charpath fill
                        showpage
                        &lt;/code&gt;
&lt;/pre&gt;
                                &lt;p&gt;
                                    This fragment uses only a subset of the language exposes the key areas of concern for simple PDF display:
                                &lt;/p&gt;
                                &lt;ol&gt;&lt;li&gt;
                                        &lt;p&gt;
                                            Since this is program code, implementation details can affect the output. As a simple, hopefully purely hypothetical example, consider how processor or compiler-specific differences in &lt;a href="http://en.wikipedia.org/wiki/Floating_point#Accuracy_problems"&gt;floating-point rounding&lt;/a&gt; could affect a complex document after many operations cause display problems such as lines which are supposed to appear joined to have visible space.
                                        &lt;/p&gt;
                                        &lt;p&gt;
                                            As the full language is far more complicated than the subset above, there are many variations on this theme. Fortunately the mainstream implementations have generally converged on reliable inter-operability but you are still likely to need a copy of Acrobat if you receive content from a wide range of sources.
                                        &lt;/p&gt;
                                        &lt;p&gt;
                                            This was particularly a problem in the past with older “print to PDF” drivers which simply took the raw PostScript which they would have sent directly to a printer and wrapped in a PDF container.
                                        &lt;/p&gt;
                                    &lt;/li&gt;
                                    &lt;li&gt;
                                        &lt;p&gt;
                                            Font choices are specified by name. The corresponding font file may be embedded within the PDF file but as system fonts are also supported, it's quite easy for authors to use special fonts and forget to embed them until the first time the PDF is opened on a system which does not have those fonts installed.
                                        &lt;/p&gt;
                                        &lt;p&gt;
                                            We've seen this somewhat frequently with academic journal articles which were created using &lt;a href="http://en.wikipedia.org/wiki/LaTeX"&gt;LaTex&lt;/a&gt; and use its fonts to display mathematical symbols. A Google search confirms that this is not an uncommon mistake as it will only be a problem when documents circulate outside the significant portion of scientific users who have the LaTeX fonts installed: &lt;a href="https://www.google.com/search?q=%2B%22Cannot+find+or+create+the+font%22"&gt;https://www.google.com/search?q=%2B%22Cannot+find+or+create+the+font%22&lt;/a&gt;
                                        &lt;/p&gt;
                                        &lt;p&gt;
                                            Additionally, the &lt;a href="http://en.wikipedia.org/wiki/TrueType"&gt;TrueType&lt;/a&gt; and particularly &lt;a href="http://en.wikipedia.org/wiki/OpenType"&gt;OpenType&lt;/a&gt; font formats are by necessity quite complex to deal with the range of human writing systems. Again, this is an area of potentially significant difference between implementations and, particularly for &lt;a href="http://en.wikipedia.org/wiki/Complex_script"&gt;complex scripts&lt;/a&gt; like &lt;a href="http://en.wikipedia.org/wiki/Arabic"&gt;Arabic&lt;/a&gt; or &lt;a href="http://en.wikipedia.org/wiki/Devanagari"&gt;Devangari&lt;/a&gt;, the failures can potentially lead to the text being incorrect. Fonts are versioned, so it's possible to have text which would be displayed correctly if the operating system's version of a font is used instead of the embedded version or vice versa. The more obscure the languages you work with, the more you need to have some sort of system to check for correctness.
                                        &lt;/p&gt;
                                    &lt;/li&gt;
                                &lt;/ol&gt;&lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    For simple images, PDF writers are allowed to use &lt;a href="http://en.wikipedia.org/wiki/PDF#Raster_images"&gt;a number of encodings&lt;/a&gt; and over the years various image formats have been added, all of which have require full software support:
                                &lt;/p&gt;
                                &lt;p&gt;
                                    &lt;a href="http://en.wikipedia.org/wiki/PDF#Adobe.27s_versions"&gt;http://en.wikipedia.org/wiki/PDF#Adobe.27s_versions&lt;/a&gt;
                                &lt;/p&gt;
                            &lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    Over the years, Adobe has also added many other types of rich content: audio, video, 3D imagery, etc. All of these include the full set of challenges for preserving their respective formats.
                                &lt;/p&gt;
                            &lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    Primarily for business users, Adobe has added several types of &lt;a href="http://en.wikipedia.org/wiki/PDF#Interactive_elements"&gt;interactive forms&lt;/a&gt;, which rely on several complicated specifications and have in my experience been far less supported by third-party implementations, particularly the open-source community.
                                &lt;/p&gt;
                            &lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    In PDF 1.2, support was added for &lt;a href="http://en.wikipedia.org/wiki/JavaScript"&gt;JavaScript&lt;/a&gt; as part of the forms specification. Since JavaScript is a full programming language, this means that the only way to process those actions is requires executing code in a manner consistent with the original implementation. Fortunately, this is likely to be uncommon in most preservation scenarios.
                                &lt;/p&gt;
                            &lt;/li&gt;
                            &lt;li&gt;
                                &lt;p&gt;
                                    The specification includes varying levels of encryption. It is possible to &lt;a href="http://en.wikipedia.org/wiki/Brute-force_attack"&gt;brute-force&lt;/a&gt; weak passwords and the older encryption algorithms but that might be possible and the software to do so might be difficult or illegal to obtain.
                                &lt;/p&gt;
                            &lt;/li&gt;
                        &lt;/ol&gt;&lt;p&gt;
                            In practice, many of the concerns are manageable with several precautions. If your content is not supposed to include the various rich media features the best place to start is by requiring the restricted subsets of PDF which have been developed to avoid many of these issues: &lt;a href="http://en.wikipedia.org/wiki/PDF/A"&gt;PDF/A&lt;/a&gt;, intended for preservation, and &lt;a href="http://en.wikipedia.org/wiki/PDF/X"&gt;PDF/X&lt;/a&gt;, intended for reliable graphics exchange, which do not allow the more complex features and dramatically simplify the problem. If your goal is to archive general PDFs, however, you'll need to develop a more nuanced approach to audit the various complex features to check that a document does not include content which you are unprepared for (e.g. if your content includes embedded video, your auditing script could verify that the video stream uses a long-term viable &lt;a href="http://en.wikipedia.org/wiki/Codec"&gt;codec&lt;/a&gt;).
                        &lt;/p&gt;
                        &lt;p&gt;
                            Here are some features which you might want to audit:
                        &lt;/p&gt;
                        &lt;ol&gt;&lt;li&gt;All fonts in the file are the standard core PDF fonts or embedded within the file.
                            &lt;/li&gt;
                            &lt;li&gt;All images are in the subset of formats which you are prepared to support and decode without errors.
                            &lt;/li&gt;
                            &lt;li&gt;All content streams are checked against a whitelist of supported types
                            &lt;/li&gt;
                            &lt;li&gt;The PDF is unencrypted or at least that the password is known and the file decrypts successfully
                            &lt;/li&gt;
                        &lt;/ol&gt;&lt;/blockquote&gt;</content>
    <updated>2012-07-27T00:22:41.763000+00:00</updated>
  </entry>
  <entry>
    <title>Google Analytics Site Speed considered misleading</title>
    <id>http://chris.improbable.org/2012/5/18/google-analytics-deceptive-site-speed-report/</id>
    <link href="http://chris.improbable.org/2012/5/18/google-analytics-deceptive-site-speed-report/"/>
    <summary>… maybe one of their mathematicians can explain the difference between mean and median to the marketing folks?</summary>
    <content type="html">&lt;p&gt;
                        Google Analytics has a very handy site speed feature tracking the time your users' browsers took to load the page. Unfortunately, all of the timing reports make a novice statistical mistake by reporting the &lt;a href="http://en.wikipedia.org/wiki/Average"&gt;average&lt;/a&gt; rather than more robust metrics like &lt;a href="http://en.wikipedia.org/wiki/Percentile"&gt;90th percentile&lt;/a&gt;. Many people have heard that averages are prone to &lt;a href="http://en.wikipedia.org/wiki/Outlier"&gt;outliers&lt;/a&gt; but it's easy to forget the degree to which a reported average can misrepresent something as variable as Internet traffic. Here are two pictures showing why &lt;em&gt;it's not even worth looking at the Site Speed value&lt;/em&gt;:
                    &lt;/p&gt;
                    &lt;figure&gt;&lt;img width="651" height="419" src="GA_Hazard_-_World_Map.png"&gt;&lt;figcaption&gt;
                            World map of load times for a single AJAX request: note the United States at 5.9 &lt;em&gt;seconds&lt;/em&gt;!
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img width="687" height="359" src="GA_Hazard_-_somethings_strange_is_afoot_in_New_Jersey.png"&gt;&lt;figcaption&gt;
                            Drilling down revealed the US average around .3 seconds for every state except New Jersey and even there the high average was limited to one small town with the shockingly-high average of 47 seconds! Fortunately, the data we actually need is available: the performance tab displays the distribution of timings, allowing us to see that even when considering only traffic from the same town, the vast majority (97%) of requests were loaded in a tenth of a second or less and 99% were loaded in under one second.
                        &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;
                        Since these values occurred only a single-digit number of times globally and are extremely high – does anyone really wait an hour for a web-page to load? – it's almost certain that they reflect some sort of measurement error in the browser. This is to be expected on the Internet — Flickr famously observed a reported load time which pre-dated that page being added to the site — and it's why you need to use something like a 95th percentile or histogram for any kind of real-world performance reporting so you can measure and act on values which are representative of what most of your users experience rather than wasting time chasing chimeras.
                    &lt;/p&gt;
                    &lt;p&gt;
                        In summary: 3 data points out of 213,000 are enough to skew the average by a factor of 10 or more. When using Google Analytics pretend the summary page doesn't exist and look at the performance distribution.
                    &lt;/p&gt;</content>
    <updated>2012-05-18T14:08:56+00:00</updated>
  </entry>
  <entry>
    <title>Google Analytics performance monitoring details</title>
    <id>http://chris.improbable.org/2011/12/13/google-analytics-performance-monitoring-details/</id>
    <link href="http://chris.improbable.org/2011/12/13/google-analytics-performance-monitoring-details/"/>
    <summary>The first rule of benchmarking: make sure you know what you actually measured</summary>
    <content type="html">&lt;p&gt;
                        Google Analytics has a very handy "Site Speed" report which measures the average load time for your pages as seen by your users. Unfortunately, there are two implementation details which are important to keep in mind:
                    &lt;/p&gt;
                    &lt;ol&gt;&lt;li&gt;The reports are heavily based on the average load time, which can skew the results pretty heavily: one multi-minute load time from someone in Haiti on a satellite connection outweighs quite a few users with .8 second load times. There is a bucket report but it's a bit clunky and there's no easy way to get the data out to run your own stats. Even a simple switch to using the median would make this report less ominous at first glance and it'd be a really good use for some sort of distribution graphic.
                        &lt;/li&gt;
                        &lt;li&gt;
                            &lt;p&gt;
                                The more interesting problem is that Google Analytics uses the &lt;a href="https://dvcs.w3.org/hg/webperf/raw-file/tip/specs/NavigationTiming/Overview.html"&gt;W3C Navigation Timing API&lt;/a&gt; and reports only the value of &lt;code class="language-javascript"&gt;window.performance.timing.loadEventStart - window.performance.timing.navigationStart&lt;/code&gt;. The browser won't fire the &lt;a href="https://developer.mozilla.org/en/DOM/window.onload"&gt;load event&lt;/a&gt; until all resources on the page have been loaded including external services like Google Analytics. Unfortunately, you will find situations where http://google-analytics.com/ga.js takes a considerable amount of time to respond (yesterday I had 40 second response in a &lt;a href="http://webpagetest.org"&gt;webpagetest.org&lt;/a&gt; session) and this will be reflected in your stats even if the page loaded considerably faster from the user's perspective.
                            &lt;/p&gt;
                            &lt;p&gt;
                                I'm currently experimenting with deferring loading Google Analytics until after the load event using this code. Initial testing appears promising and the elapsed time to load everything else is definitely more accurate:
                            &lt;/p&gt;&lt;script src="https://gist.github.com/1473932.js?file=deferred-ga.js"&gt;
&lt;/script&gt;&lt;details&gt;
                                Relying on NavigationTiming does mean that you won't receive any information from older versions of Internet Explorer unless they have the Google Toolbar installed, which means some of your worst-performing clients will be invisible. &lt;a href="http://code.google.com/chrome/chromeframe/"&gt;Chrome Frame&lt;/a&gt; would be my preferred response…
                            &lt;/details&gt;&lt;/li&gt;
                    &lt;/ol&gt;</content>
    <updated>2011-12-13T20:01:15+00:00</updated>
  </entry>
  <entry>
    <title>The Movie Set That Ate Itself</title>
    <id>http://chris.improbable.org/2011/10/31/the-movie-set-that-ate-itself/</id>
    <link href="http://chris.improbable.org/2011/10/31/the-movie-set-that-ate-itself/"/>
    <content type="html">&lt;div class="googlereader description" data-google-id="tag:google.com,2005:reader/item/5ba4673d932c4eb5"&gt;
                        &lt;p class="annotation"&gt;
                            “Seventy years of quotidian misery held with one waistband.”
                        &lt;/p&gt;
                        &lt;blockquote&gt;
                            &lt;blockquote&gt;
                                &lt;p&gt;
                                    Five years ago, a relatively unknown (and unhinged) director began one of the wildest experiments in film history. Armed with total creative control, he invaded a Ukrainian city, marshaled a cast of thousands and thousands, and constructed a totalitarian society in which the cameras are always rolling and the actors never go home
                                &lt;/p&gt;
                            &lt;/blockquote&gt;
                            &lt;p&gt;
                                &lt;a href="http://givemesomethingtoread.com/post/12158637111/the-movie-set-that-ate-itself"&gt;#&lt;/a&gt;
                            &lt;/p&gt;
                        &lt;/blockquote&gt;
                    &lt;/div&gt;
                    &lt;p class="bookmark-source"&gt;
                        Source: &lt;a href="http://www.gq.com/entertainment/movies-and-tv/201111/movie-set-that-ate-itself-dau-ilya-khrzhanovsky?printable=true&amp;amp;currentPage=all"&gt;http://www.gq.com/entertainment/movies-and-tv/201111/movie-set-that-ate-itself-dau-ilya-khrzhanovsky?printable=true&amp;amp;currentPage=all&lt;/a&gt;
                    &lt;/p&gt;</content>
    <updated>2011-10-31T13:24:21+00:00</updated>
  </entry>
</feed>
