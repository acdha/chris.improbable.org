<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>feature: Cooling the datacenter: "hippie engineering" meets modern IT</title>
        <meta name="viewport" content="width=device-width,initial-scale=1.0">
        <link rel="stylesheet" href="/static/css/main.min.css" type="text/css" media="all">
        <!--[if lte IE 8]>
            <link
                rel="stylesheet"
                href="/static/css/ie-fixes.css"
                type="text/css"
                media="all"
            />
            <script src="/static/js/html5shiv.js"></script>
        <![endif]-->

        <meta http-equiv="last-modified" content="Tue, 13 Apr 2010 17:00:47 GMT">
        <link rel="alternate" type="application/atom+xml" title="All Posts (Atom)" href="/feeds/all.atom">
        <link rel="alternate" type="application/rss+xml" title="All Posts (RSS)" href="/feeds/all.rss">
        <link rel="icon" type="image/jpeg" sizes="287x287" href="/static/img/favicon-287x287.jpg">
        <link rel="icon" type="image/jpeg" sizes="128x128" href="/static/img/favicon-128x128.jpg">
    </head>
    <body class="blog post_detail" itemscope itemtype="http://schema.org/BlogPosting">
        <nav id="site-nav">
            <header id="about">
                <h1><a href="/about/">Chris Adams</a></h1>
                <h2>Programmer, cyclist, photographer</h2>
            </header>
            <ul class="links">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/about/">About</a>
                </li>
                <li>
                    <a href="/feeds/all.atom" title="Site Feed">Site Feed</a>
                </li>
            </ul>
            <ul class="social-networks">
                <li>
                    <a rel="me" href="https://code4lib.social/@acdha">Mastodon</a>
                </li>
                <li>
                    <a href="https://github.com/acdha" rel="me">Github</a>
                </li>
                <li>
                    <a href="https://bitbucket.org/acdha" rel="me">Bitbucket</a>
                </li>
                <li>
                    <a href="https://pinboard.in/u:acdha/" rel="me">Pinboard</a>
                </li>
                <li>
                    <a href="https://www.flickr.com/photos/acdha" rel="me">Flickr</a>
                </li>
                <li>
                    <a href="https://www.linkedin.com/in/acdha" rel="me">LinkedIn</a>
                </li>
            </ul>
            <div id="site-search"></div>
        </nav>
        <section id="main">
            <article class="post">
                <header>
                    <meta itemprop="dateCreated" content="2010-04-13T13:00:47-04:00">
                    <meta itemprop="dateModified" content="2010-04-13T13:00:47-04:00">
                    <time class="date" itemprop="datePublished" datetime="2010-04-13T17:00:47+00:00">Apr 13</time>
                    <h2 class="" itemprop="title">feature: Cooling the datacenter: "hippie engineering" meets modern IT</h2>
                </header>

                <div class="body" itemprop="articleBody"><div class="googlereader description" data-google-id="tag:google.com,2005:reader/item/2fe5afc9d90b3704">
                        <blockquote>
                            <a href="http://arstechnica.com/business/news/2010/04/hippie-engineering-meets-information-technology.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss"><img vspace="4" hspace="4" border="0" align="right" src="http://static.arstechnica.com/assets/2010/04/feat_environmental_datacenter_list-thumb-230x130-13305-f.jpg"></a>
                            <p>
                                Tourists visiting Newcastle upon Tyne are more likely to pack a warm wool sweater than a beach blanket. In northern England, swimsuit season brings summer rains and chilly temperatures. Yet according to the Palo Alto, CA-based Hewlett-Packard, you won't find a better spot for a data center.
                            </p>
                            <p>
                                "The cool location is very attractive. We will probably only run the auxiliary cooling devices three days a year," says Ed Kettler, a fellow at Hewlett-Packard (HP).
                            </p>
                            <p>
                                In February the company opened <a href="http://www.youtube.com/watch?v=QsIyzdva780">a facility</a> that pulls sea air through seven-foot intake fans. The first story of the building is used to channel air. "We built a second story for the data center, and put intake fans on the first level," says Kettler. "We basically have a twelve-foot raised floor."
                            </p>
                            <p>
                                Typical data centers look nothing like this. Most use a three-foot raised floor to circulate cold air. Beneath perforated floor tiles, an air conditioner often runs day and night. Even with air-side economization—a cooling system that brings in outside air—chillers are usually needed to cool the air after intake. Inefficient compressors force the air upwards through tiny holes in the floor tiles.
                            </p>
                            <p>
                                "Our system is more efficient because it uses fans," says Kettler. If industry standards change, allowing servers to run a degree or two hotter, Kettler says he will never have to run mechanical chillers.
                            </p>
                            <p>
                                As much as half the energy used by a data center goes towards cooling. While total power use is beginning to drop thanks to equipment efficiency, data centers still consume more energy than all color televisions in the US combined. Sixty-one billion kilowatt-hours (kWh) were consumed in 2006, representing 1.5% of national energy use.
                            </p>
                            <h3>
                                Back to the future
                            </h3>
                            <p>
                                This is why a handful of high tech companies are going retro, and revamping historic building designs that pre-date punch cards and magnetic strips (not to mention blade servers). The designs borrow from old bungalows, Northern California communes, and even Pre Columbian pueblos and ancient Roman villages. In the spirit of permaculture—a design philosophy that uses wind and sunlight for free cooling and light—electricity use is on the decline.
                            </p>
                            <p>
                                "A cool, breezy location, and a smart building design will make a big difference," says Chris Page, Director of Climate and Energy Strategy for Yahoo!.
                            </p>
                            <p>
                                Yahoo is scheduled to open a Lockport, New York data center in September—called the "Yahoo Compute Coop," the patent-pending design harnesses prevailing winds for cooling. Like a chicken coop, thermal convection moves air through the building, and coolers will only be run a handful of days each year.
                            </p>
                            <p>
                                The design is surprisingly simple. While wind power requires gusty hilltops and canyons, not to mention windmills, IT permaculture—to coin a term where one is needed—doesn't aim to make electricity from the elements. The goal is to simply use the natural features present on a piece of property to help reduce reliance on the grid.
                            </p>
                            <p>
                                "Unlike wind power, our designs don't require strong prevailing winds," says Page.
                            </p>
                            <p>
                                Similar design efforts are under way at Stanford University in Palo Alto, CA, and at the National Renewable Energy Lab in Golden, CO, which plans to open a new data center in June.
                            </p>
                            <p>
                                "Nowadays we live in sealed little boxes," says Paul Torcellini, an engineer at the National Renewable Energy Lab (NREL). "But before 1930, people had to design buildings to work with their environment—to make the most of natural air conditioning and lighting."
                            </p>
                            <p>
                                This is why old bungalows have large, covered porches and breezeways. Deciduous trees and vines can be planted along south facing walls, blocking sun in the summer, and allowing light to pass through windows in the winter. In some designs, south facing windows channel light to heat-absorbent rock walls and floors. Thanks to similar tactics, Torcellini's <a href="http://www.nrel.gov/sustainable_nrel/rsf.html">new $63 million building</a> will run entirely on solar. "Ten years ago, no one thought a data center could do this—the average solar array would barely make a dent in typical energy consumption," says Torcellini.
                            </p>
                            <p>
                                While solar has improved since then, the number of panels needed for a traditional data center still exceeds most budgets and space restrictions. This is why cutting energy use is so important, says Torcellini. "By reducing demand with climate friendly designs, we can power the entire data center and office with the solar energy we make on site," he says.
                            </p>
                            <p>
                                Designed to be one of the most efficient offices in the world, the building uses half the energy of a typical facility, even despite housing a 3,000 square foot data center. The building will soon have Platinum level Leed Certification—a rating offered by the Washington DC-based Green Building Council.
                            </p>
                            <h3>
                                High Tech Hippies
                            </h3>
                            <blockquote>
                                Drawing from a design in Utah's Zion National Park, evaporative cooling also helps Torcellini cool his data center.
                            </blockquote>
                            <p>
                                Trellised over a south-facing deck, leafy kiwi plants are used for passive cooling and heating at the Occidental Arts and Ecology Center—an intentional community and permaculture design school based in Sonoma, CA. The leaves block sun in the summer, and fall to the ground in the winter, allowing light and warmth through the sliding door.
                            </p>
                            <p>
                                Just down the coast in Bolinas, James Stark evaluates a home he built at the Regenerative Design Institute. "We planted citrus on the Southside of our office to help block the sun, and also because citrus likes the heat from the walls," says Stark, who teaches permaculture. The house uses earthen walls to retain heat—at least on the south face. Light straw clay is used on the north side of the building to provide insulation.
                            </p>
                            <p>
                                "It's not hard to make similar good design features work in the context of a data center," says Page. "There is a tendency to plop down data centers without looking at the climate and energy factors, but you save a lot of money by working with the local environment."
                            </p>
                            <p>
                                Absorbing and blocking light is a classic permaculture approach. This is why the National Renewable Energy Lab built their data center into a slope on the north side of the building. On the opposite side of the building, offices are entirely above ground, and lighted with sun. Near the top of the building, a row of glass channels light to a reflective, white ceiling.
                            </p>
                            <p>
                                "You want this level of windows as high as possible so that they get the most sun to the ceiling," says Torcellini. As the sun hangs lower in the sky during the winter, tiny louvers were installed to collect and channel the rays. In the summer, the light is scattered to prevent heating.
                            </p>
                            <p>
                                Pre Columbian Anasazi villages harnessed similar strategies. Built in the upper sides of high desert cliffs, the sun warmed rock dwellings directly in the winter. Rays angled towards the bottom side of the rock face in the summer, missing the village abodes. Rome and Greek communities were similarly heated with passive solar, and viewed a failure to do so as uncivilized. The playwright Aeschylus suggested only barbarians "lacked knowledge of houses turned to face the winter sun."
                            </p>
                            <p>
                                Aeschylus doesn't use the phrase "permaculture"—neither do Page, Torcellini and Kettler, for that matter. Yet their buildings are reminiscent of the "hippie engineering" fad that borrows from ancient and historic designs. Popularized in Australia and Northern California during the '70s, permaculture certifications are awarded at green design institutes throughout the San Francisco Bay Area. Permaculture refrigerators harness cold air from shallow holes dug beneath the kitchen floor. A rectangular pipe connects the earth to a ceiling vent, where a fan pulls the constant cold air supply upwards. After installing shelves, screens, and a refrigerator door, food can be kept at 40 degrees.
                            </p>
                            <h3>
                                Bringing permaculture to the datacenter
                            </h3>
                            <p>
                                Torcellini and his colleagues used a similar strategy when designing their data center—even though at a much larger scale. Air for the building's air conditioning system is pulled from a cool basement maze of concrete and stone. "We call it the labyrinth," says Torcellini. "Cement walls trap the natural cold in the earth, and air for the data center is filtered through."
                            </p>
                            <p>
                                In the 1920s similar "earth tunnels" were installed in Washington DC. Like in a cave, the air in the earth is constantly cold—even during the summer. "It's the same principle as with the fridge," says Torcellini. "The air just needs a long path to travel to pick up the cool."
                            </p>
                            <p>
                                Drawing from a design in Utah's Zion National Park, evaporative cooling also helps Torcellini cool his data center. At the Visitor's center, hot desert air is funneled through chimneys that contain moist water membranes. As air passes through the wet layer, it cools, becomes dense, and falls. The momentum circulates a breeze through the building—all without the use of electricity.
                            </p>
                            <p>
                                "It's basically like a swamp cooler that uses no energy," says Torcellini, who helped design the Zion towers. "Because of the chimney height, the flow of cool air is quite strong." In the scorched desert of Southern Utah, summertime temperatures reach 110 degrees. Yet the towers push 5,000 cubic feet per minute of cool air into the building.
                            </p>
                            <p>
                                To integrate this model into the data center, Torcellini swapped tall chimneys for solar powered fans, and installed evaporative pads into the air ducts. "You could design chimneys like those in Zion for use in a data center, although it wouldn't work in humid areas. It's not the heat that makes the system work, it's the dry air," says Torcellini. By relying on fans, the data center's duct system works all year, regardless of changes in humidity.
                            </p>
                            <p>
                                "The point is to work with Mother Nature rather than against her," says Kettler.
                            </p>
                            <p>
                                HP's facility combines a host of additional green features. It stores rainwater from the rooftop in underground cisterns, and then uses it in the data center's humidification system. Warm air from server racks is used to heat the building, and a reflective roof lowers inside temperatures. The company replaced parking lots with native trees and grass, and cracked concrete was removed for wetlands restoration. "We tried to return the area to its natural state, so the water seeps into the ground through grass and plants, like it would have before the building was constructed," says Kettler.
                            </p>
                            <p>
                                Individually, these systems may only solve part of the cooling problem, but combined, they have a big impact. Permaculture data centers say they will only use coolers three days a year—some may not ever run auxiliary chillers. Rainwater harvesting and natural lighting will also have a big impact on utility bills.
                            </p>
                            <p>
                                "You can build the initial building a lot cheaper and run it using less electricity," says Page. "Green design is often cost effective design."
                            </p>
                            <h3>
                                It's how you use what you've got
                            </h3>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/internal-clear.jpeg">
                                </div>
                                <div>
                                    <div>
                                        Stanford's datacenter
                                    </div>
                                </div>
                            </div>
                            <p>
                                To help IT companies locate the best climate for their data center, the Beaverton, Oregon-based nonprofit Green Grid launched a series of <a href="http://www.thegreengrid.org/en/Global/Content/TechnicalForumPresentation/FreeCoolingToolPowerConfigurationEfficiencyEstimator">interactive cooling maps</a> last year.
                            </p>
                            <p>
                                Engineers pick a location, and then modify temperatures and humidity conditions to see how many annual hours of "free cooling" are offered in the area. "You can also type in your zip code, and search for surrounding areas," says Roger Tipley, Senior Engineering Strategist, at HP, and a board member at Green Grid.
                            </p>
                            <p>
                                Released last week, a "Power Configuration Efficiency Estimator" helps evaluate power distribution topology choices. This new tool helps engineers assemble their data centers efficiently. By reducing cooling needs, data centers can rely on outdoor air for more hours each year.
                            </p>
                            <p>
                                For companies stuck in Las Vegas, Nevada, or Houston, Texas, Torcellini says there is no reason to lose hope. "You can work with the conditions in any area; even the designs are different," he says. Denver hits 100 degrees during the summer, yet the National Renewable Energy Lab harnesses cool west winds from the flatlands for much of the year.
                            </p>
                            <p>
                                Engineers working on a new Stanford data center are currently choosing between wind capture and heat harvesting technologies, and they say every design has its flaws and benefits. Hills near campus pick up cool coastal breezes, and Stanford identified a spot where the winds blow in consistent patterns. "One option is to structure the data center like a chimney, and pull the air through to the servers from the hill," says Joyce Dickerson, director of sustainable IT at Stanford. Hot air would be released to the sky through air vents above racks, and a spine-like conduit would pull cool air through the center of the building.
                            </p>
                            <p>
                                "We think it's a great design," says Dickerson. "But we may have an even more efficient option." Locating the new data center closer to campus will lower the transportation footprint, and make the facility more accessible to researchers.
                            </p>
                            <p>
                                Heat may also be directed from the back of the servers to the central campus utility. In this case, water coils would run through the back of server racks; on contact with the coils, heat would transfer, preventing room temperatures from rising. "The plan is to ultimately provide all heating and cooling on campus with water, so it would be easy to integrate this system into the data center," says Dickerson.
                            </p>
                            <p>
                                These efforts are applauded by sustainability consultants. Both heat harvesting and wind-based designs represent an unprecedented level of integration. "Facilitating temperature flow at this level is novel," says Mark Bramfitt, a San Francisco-based consultant who specializes in energy efficient retrofits for data centers. "The problem in most data centers is that the hot and cold air mix, and this wastes a lot of energy," says Bramfitt.
                            </p>
                            <p>
                                Wind harvesting designs get around this with clever ducting and venting systems. Heat from servers is channeled away from machines. Cool air is brought right to the servers. Some designs even use wireless temperature sensors to direct air towards the hottest racks.
                            </p>
                            <h3>
                                Old school data centers and permaculture
                            </h3>
                            <blockquote>
                                Old buildings weren't designed to let air in, and the payback time on retrofits is often three to five years.
                            </blockquote>
                            <p>
                                Applying permaculture techniques doesn't require wind harvesting and passive solar. Even data centers that opt against wind harvesting can benefit from better heat and air circulation, says Bramfitt. "You can get nice cool air into the data center any number of ways, but this doesn't mean it will be blown around efficiently," he says.
                            </p>
                            <p>
                                As most data centers push air through perforated floor tiles, the room is cooled from the ground up. Racks are often fifteen degrees hotter at the top than the bottom, and air is wasted cooling the room itself, rather than the machines. This is why the average data center spends 15 percent of its energy blowing air around, says Bramfitt.
                            </p>
                            <p>
                                Thermal convection can be integrated into almost any cooling model, and ducting systems can be used to separate hot and cool air. Stanford has recently introduced tiles that contain scoops. "It gets the cold right to the machines, and pushes air to the top of the racks more quickly," says Dickerson. The center also installed the same ceiling tiles used in clean rooms. Strip curtains are another way to direct air, and white racks can be used to reflect light. Delivering cold air right to machines can be as simple as moving perforated tiles to the right place. "You don't have to harness sea winds to manage your air flow better," says Bramfitt.
                            </p>
                            <p>
                                These simple sustainability efforts are perhaps more realistic—at least for the average data center. "Old buildings weren't designed to let air in, and the payback time on retrofits is often three to five years," says Bramfitt. Air side economization—though a much smaller-scale effort than wind harvesting—is still too expensive for the data center dinosaurs. The original concept was to isolate computers as far away from the elements as possible. Tucked away in the heart of the building, behind thick walls and layers of protection, it's hard to bring outside air into the most retro of retrofits.
                            </p>
                            <p>
                                As far as integrating IT-permaculture techniques, it's also difficult to predict the temperature changes a company can expect. "Measuring convection and air flow is hard," says Torcellini, "and it's hard to say we lost x degrees because of cooling from the basement labyrinth or the wind from the flat lands."
                            </p>
                            <p>
                                The biggest hurdle is measuring airflow. "It's like trying to figure out how much wind comes through your window," says Torcellini. "Just putting your hand up to feel the breeze blocks some of the air flow." Measurements in the Zion chimneys were complicated by instrumentation that blocked breezes, and while tracer gasses are sometimes used to visualize the flow of air, these don't always work in open systems.
                            </p>
                            <p>
                                Like the communes of Northern California and the villages of ancient times, the proof is more in the pudding than the metric. "There is no formula for permaculture—you just design buildings to work with the land, and modify until you have something that works" says Stark, who teaches this philosophy to his permaculture students.
                            </p>
                            <p>
                                This is one way that the National Renewable Energy Lab's new data center is perhaps more high tech than hippie. Despite the difficulty of measuring integrated systems, they are making every effort to try. Meters have been installed on evaporative pads and fans, and each cooling component will be measured as the seasons change. By the end of the year, researchers should know how these systems work together to ease energy demand.
                            </p>
                            <p>
                                The bottom line is that companies need to be more proactive about their data center footprint, says Torcellini. "A lot of companies outsource to server farms, believing that if the data center is no longer part of their utility bill, it's not their problem," he says. "We designed our building to show that the highest energy efficiency standards can be met, even with an in-house data center."
                            </p>
                            <p>
                                <a href="http://arstechnica.com/business/news/2010/04/hippie-engineering-meets-information-technology.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss&amp;comments=1#comments-bar">Read the comments on this post</a>
                            </p>
                        </blockquote>
                    </div>
                    <p class="bookmark-source">
                        Source: <a href="http://arstechnica.com/business/news/2010/04/hippie-engineering-meets-information-technology.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss">http://arstechnica.com/business/news/2010/04/hippie-engineering-meets-information-technology.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</a>
                    </p>
                </div>
            </article>

            <nav id="post-nav">


            </nav>
        </section>

        <footer id="site-footer" class="nocontent">
            <p>
                This site is purely my personal work and does not reflect the
                views of my employer.
            </p>
            <p class="license">
                <a class="icon" rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width: 0" src="https://i.creativecommons.org/l/by-sa/3.0/88x31.png"></a>
                This work is licensed under a
                <a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported
                    License</a>.
            </p>
        </footer>

        <script async src="/static/js/common.js"></script>
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/prettify/r298/run_prettify.min.js" integrity="sha256-hj+5FRlAuvAFANiefn0PpJYCkV1X4QT9EgiPd+6QnCw=" crossorigin="anonymous"></script>
    </body>
</html>
