<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>
            feature: How robots think: an introduction
        </title>
        <meta name="viewport" content="width=device-width,initial-scale=1.0">
        <link rel="stylesheet" href="/static/css/main.min.css" type="text/css" media="all"><!--[if lte IE 8]>
            <link rel="stylesheet" href="/static/css/ie-fixes.css" type="text/css" media="all">
            <script src="/static/js/html5shiv.js"></script>
        <![endif]-->
        <meta http-equiv="last-modified" content="Tue, 16 Mar 2010 05:30:38 GMT">
        <link rel="alternate" type="application/atom+xml" title="All Posts (Atom)" href="/feeds/all.atom">
        <link rel="alternate" type="application/rss+xml" title="All Posts (RSS)" href="/feeds/all.rss">
        <link rel="icon" type="image/jpeg" sizes="287x287" href="/static/img/favicon-287x287.jpg">
        <link rel="icon" type="image/jpeg" sizes="128x128" href="/static/img/favicon-128x128.jpg">
        <script type="application/javascript">
var _prum={id:"5166be01e6e53d1007000001"};var PRUM_EPISODES=PRUM_EPISODES||{};PRUM_EPISODES.q=[];PRUM_EPISODES.mark=function(b,a){PRUM_EPISODES.q.push(["mark",b,a||new Date().getTime()])};PRUM_EPISODES.measure=function(b,a,b){PRUM_EPISODES.q.push(["measure",b,a,b||new Date().getTime()])};PRUM_EPISODES.done=function(a){PRUM_EPISODES.q.push(["done",a])};PRUM_EPISODES.mark("firstbyte");(function(){var b=document.getElementsByTagName("script")[0];var a=document.createElement("script");a.type="text/javascript";a.async=true;a.charset="UTF-8";a.src="//rum-static.pingdom.net/prum.min.js";b.parentNode.insertBefore(a,b)})();
        </script>
    </head>
    <body class="blog post_detail" itemscope itemtype="http://schema.org/BlogPosting">
        <nav id="site-nav">
            <header id="about">
                <h1>
                    <a href="/about/">Chris Adams</a>
                </h1>
                <h2>
                    Programmer, cyclist, photographer
                </h2>
            </header>
            <ul class="links">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/about/">About</a>
                </li>
                <li>
                    <a href="/feeds/all.atom" title="Site Feed">Site Feed</a>
                </li>
            </ul>
            <ul class="social-networks">
                <li>
                    <a href="https://github.com/acdha" rel="me">Github</a>
                </li>
                <li>
                    <a href="http://bitbucket.org/acdha" rel="me">Bitbucket</a>
                </li>
                <li>
                    <a href="http://delicious.com/acdha" rel="me">del.icio.us</a>
                </li>
                <li>
                    <a href="http://twitter.com/acdha" rel="me">Twitter</a>
                </li>
                <li>
                    <a href="https://plus.google.com/116562742092842686896?rel=author" rel="me">Google+</a>
                </li>
                <li>
                    <a href="http://www.flickr.com/photos/acdha" rel="me">Flickr</a>
                </li>
                <li>
                    <a href="http://www.linkedin.com/in/acdha" rel="me">LinkedIn</a>
                </li>
                <li>
                    <a href="http://connect.garmin.com/explore?owner=acdha" rel="me">Garmin Connect</a>
                </li>
            </ul>
            <div id="site-search"></div>
        </nav>
        <section id="main">
            <article class="post">
                <header>
                    <meta itemprop="dateCreated" content="2010-03-16T01:30:38-04:00">
                    <meta itemprop="dateModified" content="2010-03-16T01:30:38-04:00"><time class="date" itemprop="datePublished" datetime="2010-03-16T05:30:38+00:00">Mar 16</time>
                    <h2 itemprop="title">
                        feature: How robots think: an introduction
                    </h2>
                </header>
                <div class="body" itemprop="articleBody">
                    <div class="googlereader description" data-google-id="tag:google.com,2005:reader/item/cbd4d344f08078d9">
                        <blockquote>
                            <a href="http://arstechnica.com/science/news/2010/03/how-robots-think.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss"><img vspace="4" hspace="4" border="0" align="right" src="http://static.arstechnica.com/assets/2010/03/robots_think_list-thumb-230x130-12610-f.jpg"></a>
                            <p>
                                A future full of helpful robots, quietly going about their business and assisting humans in thousands of small ways, is one of technology's most long-deferred promises. Only recently have robots started to achieve the kind of sophistication and ubiquity that computing's pioneers originally envisioned. The military has hundreds of UAVs blanketing the skies above Iraq and Afghanistan, and Roombas are vacuuming living rooms across the country. At the bleeding edge, there's the DARPA Grand Challenge in 2005. This grueling, 140-mile, no-humans-allowed race through the desert showcased full-sized, completely autonomous robot cars that could navigate across rugged desert terrain, avoiding rocks and cliffs and cacti in a race for a $2 million cash prize. The follow-on 2007 Urban Challenge went even further, with the robotic competitors required to drive alongside humans on crowded roads, recognizing and avoiding other cars and following the rules of the road. Suddenly, the robotic future doesn't look so far off.
                            </p>
                            <p>
                                In some ways, the remarkable thing is that it took so long to get here. In the 1960's, researchers in artificial intelligence were boldly declaring that we'd have thinking machines fully equivalent to humans in 10 years. Instead, for most of the past half-century, the only robots we saw outside of movies and labs were arms confined to factory floors and were remotely operated by humans. Building machines that behaved intelligently in the real world was harder than anyone imagined.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/dgc05.jpg">
                                </div>
                                <div>
                                    <div>
                                        Robot cars lined up at the starting line for the 2005 DARPA Grand Challenge race.
                                    </div>
                                </div>
                            </div>
                            <p>
                                The biggest challenge for robots then and now lies in making sense of the world. With perfect information, many of the hardest problems in robotics would be nearly trivial. We've gotten very good at building and actuating robots, but in order for them to use their abilities to the fullest they need to make sense of their surroundings. A robot car has to know where the road is and where other cars and people are. A robot servant needs to be able to recognize household items.
                            </p>
                            <p>
                                Today's robots are starting to be able to make these difficult determinations. The question we're here to answer is: how? What allowed robots to go from blind, dumb, immobile automatons to fully autonomous entities able to operate in unstructured environments like the streets of a city? The most obvious answer is Moore's Law, and it has certainly been a huge factor. But raw processing power is useless without the right algorithms. A revolution has taken place in the robotics world. By embracing uncertainty and using the tools of probability, robots are able to make sense of their surroundings like never before.
                            </p>
                            <p>
                                In this article, we'll explore how robots use their sensors to make sense of the world. This discussion applies mostly to robots that carry an internal representation of the world and act according to that representation. There are lots of successful robots that don't do such "thinking": the military's UAVs are mostly remotely piloted, linked by an electronic tether to human eyes and brains on the ground. The Roomba does its job without building a map of your house; it just has a series of simple behaviors that are triggered by timing or bumping into things. These robots are very good at what they do, but to autonomously carry out more complicated tasks like driving, a robot needs to have some understanding of the world around it. The robot needs to know where it is, where it can and can't go, and decide what to do and where to go. We'll be discussing how modern robots answer these questions.
                            </p>
                            <h3>
                                Sensing and Probability
                            </h3>
                            <p>
                                As it turns out, the big challenge in many robotics applications is the same: it's easy to do the right thing, but only if you know what the right thing is. We've known how to steer a car automatically for a long time. What's hard is knowing where the road is and whether that shape by the road is a fire hydrant you can ignore or a child about to run across the street. To operate in an unstructured environment, a robot needs to use sensing to understand the state of the world relative to itself. Sensing is the key to successful robots, and probability is the key to successful sensing.
                            </p>
                            <p>
                                Sensing is difficult because the world is a complicated, unpredictable place. Remember that the robot doesn't get to "see" reality directly. It can only take measurements through its sensors, which don't perfectly reflect the true state of the world. Just because your sensor tells you something doesn't mean it's true. For example, GPS position measurements can jump by several meters, even when the receiver is stationary. Some things aren't even possible to measure directly; if you're trying to distinguish between a person and a cactus, there's no sensor that directly measures "humanness." You have to look at different measurable properties like shape and size and so on to infer if you're seeing a person.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/sensing.png">
                                </div>
                                <div>
                                    <div>
                                        A robot doesn't directly "see" the true state of reality. Instead it must infer the state from noisy sensor measurements.
                                    </div>
                                </div>
                            </div>
                            <p>
                                Robotic sensing is like Plato's allegory of "the cave": there are people moving outside, but our robot is the prisoner chained to the wall, only able to see the shadows those people cast. The movements of the people outside are the true state, and the shadows are our robot's sensor measurements: we have to infer what's actually happening by observing the shadows. This process of inferring the true state of the world, the process of updating what we believe is true based on what our sensors tell us, is called state estimation.
                            </p>
                            <p>
                                The idea behind state estimation is straightforward. Say you're someone living in the US and you don't get to directly observe events happening in London. So you take "sensor measurements" of London by reading some US-based newspapers. Based on past experience, The <em>New York Times</em> is more likely to have an accurate report than a supermarket tabloid, so you trust the different news sources differently. Let's say you're trying to estimate the weather in London. The <em>Times</em> tells you that it's raining in London, and you know that it could very well be raining there. The <em>Times</em> could be wrong, but it generally doesn't report "rain" when the true state is "no rain." Since a trustworthy source is reporting something that that you already believe is possible, you'd probably be fairly confident that it is in fact raining in London.
                            </p>
                            <p>
                                Now say that you're trying to figure out if aliens have landed in London. If you read in the supermarket tabloid that an alien spaceship had landed in London you probably wouldn't believe it, because your experience tells you the tabloid is likely to report "aliens" when the actual state is "no aliens," and your previous belief is strongly in favor of "no aliens." However, the <em>New York Times</em> is unlikely to report "aliens" when the true state is "no aliens," so if the <em>Times</em> reported "aliens" you'd be skeptical still, but it will change your belief more than the tabloid. And if you keep seeing reports of an alien spaceship in London from trustworthy sources, your belief might go from near certainty in "no aliens" to being unsure about "aliens" versus "no aliens." or even certainty in "aliens."
                            </p>
                            <p>
                                The example above captures the two things we need to quantify in order to update our state estimate using a sensor measurement. We need a way of capturing the fact that we can be unsure about the true state, and we need to specify how much we trust each sensor. We do this by using the mathematics of probability. Instead of having a single value for each state variable, the robot maintains a probability distribution over possible state values. These probabilities represent how strongly we believe each possible value is the true value. This is our belief over the state, and it gives us a way of quantifying uncertainty.
                            </p>
                            <p>
                                To handle the trust problem, we make a probabilistic model of the sensor. Instead of explicitly stating how much to trust a sensor, the model tells us how likely we are to see a particular sensor reading for a given value of the true state. In other words, for each possible value of the state, how much does our sensor reading support the belief that this state is the true state? Now all we need to know is how to use a sensor measurement to update our belief. The tool that allows us to do this is an equation called Bayes' Rule.
                            </p>
                            <h3>
                                Bayes' Rule
                            </h3>
                            <p>
                                By itself, Bayes' Rule is a simple equation relating the probabilities of two variables, X and Z:
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/bayesRule.png">
                                </div>
                                <div>
                                    <div>
                                        Bayes' Rule gives us a relationship between our belief over the state before and after a sensor measurement is taken.
                                    </div>
                                </div>
                            </div>
                            <p>
                                The equation has three terms that matter. The term on the left side of the equation, P(X = x | Y = y), is called the posterior, and it tells us how likely it is that the variable X takes on the value x given that we observed the variable Z had the value z. If X is a state variable of interest and Z is a sensor measurement that returned z, the posterior probability is our belief that the true value of X is x. In our newspaper example it represents what we believe about the London weather after reading the paper.
                            </p>
                            <p>
                                The term P(Z = z | X = x) is the probability of seeing Z = z if X = x. This is our model of the sensor: how likely were we to have seen this measurement for a possible value of the state. Like in the newspaper example, it represents our level of trust in each paper: how likely each paper was to report the correct state in London. The third term, P(X = x) is called the prior, and it represents our original belief that X = x before taking the measurement. The term n is a normalizing constant computed to make the sum of the probabilities of all the possible values of X add up to one.
                            </p>
                            <p>
                                The end conclusion of Bayes' Rule is this: how much we believe a certain state value is the true value is simply how strongly we believed in it before times how much the sensor reading supports that particular value. There are subtleties in how to represent the probabilities of various states, but that's the basic principle. For more details on this I highly recommend reading Sebastian Thrun's textbook, <em>Probabilistic Robotics</em>.
                            </p>
                            <p>
                                The result of using Bayes' Rule is very powerful: it tells us exactly how likely each possible state is given our sensor readings. As long as we have our model for the sensor (matching the probability of the readings we get to the true state), we know exactly how to update our posterior probability for our state for any sensor. Moreover, we can use the posterior from one measurement as the prior for another, performing sequential updates as time passes or combining the measurements from multiple different sensors. Finally and most importantly, Bayes' Rule isn't limited to sensors that make direct measurements. Our sensor probability simply captures the relationship between two variables. This means that we can use pretty much any source of information to update our estimate, and Bayes' Rule tells us how to extract the maximum amount of information from what the source tells us. The specific implementations of how robots interact with the world differ greatly, but Bayesian probability is how nearly all of them "see" the world. This single-line equation underpins much of modern robotics, and is used to answer our big three questions: Where am I, what's around me, and what do I do?
                            </p>
                            <h3>
                                Navigation
                            </h3>
                            <p>
                                Knowing your own location and orientation is pretty critical to many robots. One way to find out where you are is to use GPS and inertial sensors like accelerometers. If GPS is unavailable, for example in indoor environments, a robot might find its way using cameras and machine vision. In each case, probability and Bayes' Rule are critical to accurate navigation.
                            </p>
                            <h4>
                                Fusing GPS and Inertial Sensors
                            </h4>
                            <p>
                                Consider the needs of an autonomous car (or indeed any outdoor robot). We need an estimate of our vehicle's position, so that we can figure out where to drive, and orientation, so that we know where our cameras or other sensors are pointing. GPS is great at giving an accurate absolute position, but it doesn't give us orientation unless we're moving and it's noisy. It's also unavailable if our robotic car drives under an overpass or through dense downtown areas with no clear view of the satellites.
                            </p>
                            <p>
                                Inertial sensors (gyros and accelerometers) are another way to get position and orientation: if we know where we started we can integrate acceleration to get position and angular rates to get orientation. This will give us smooth positions that don't jump around and don't depend on a satellite signal. However, after a while, small errors add up and eventually our estimated position and orientation will drift farther and farther from the truth. What we would like is some way to utilize both sensors to balance each other's weaknesses.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/GPS_IMU_paths.png">
                                </div>
                                <div>
                                    <div>
                                        Combining GPS measurements with inertial measurements produces a more accurate estimate of the vehicle's positions than either alone.
                                    </div>
                                </div>
                            </div>
                            <p>
                                Instead of figuring out some relative weighting between the positions we get from the inertial sensing and the GPS, we can simply use Bayes' Rule. A probabilistic sensor model allows us to capture the errors inherent in each sensor, and Bayes' Rule tells us how to use each measurement to update our state estimate. We can then perform sequential, independent Bayesian updates. First we use the inertial sensors to get a new position and orientation estimate, then use that as the prior to perform another update with the GPS.
                            </p>
                            <p>
                                The resultant position estimate we get combines the best of both sensors: the inertial information smoothes out the GPS's noise, and the GPS keeps the final position estimate from drifting off. If we don't get GPS for a while we can just use inertial sensing until the satellites come back. The picture below shows an example of these sequential updates (since it's hard to draw probabilities over all possible locations, we just show the most likely location after each update).
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/GPS_IMU_update.png">
                                </div>
                                <div>
                                    <div>
                                        To combine GPS measurements with inertial ones, we can simply do sequential measurement updates, using the posterior from the inertial measurement as the prior for the GPS.
                                    </div>
                                </div>
                            </div>
                            <p>
                                This form of Bayesian sensor fusion for navigation is used very widely: not only on all of the DARPA Grand Challenge vehicles, but also on most modern aircraft, ships, submarines, and pretty much anything else that needs to know where it is. In fact, this kind of Bayesian navigation was invented for the Apollo space program in the '60s, where it was known as a Kalman filter and used to combine the inputs of the Apollo space capsule's gyros with measurements from a star tracker.
                            </p>
                            <h4>
                                Camera-Based Navigation
                            </h4>
                            <p>
                                Now let's consider how cameras can be used for navigation. Here, we need to know how likely we are to see a particular view from each possible location. In human terms, imagine you looked out the window and saw the Statue of Liberty. Now you're unlikely to see the Statue of Liberty if you're in London or Austin or San Francisco, but the probability is pretty high in New York (and also Vegas, although really you can see almost every world landmark in Vegas). Based on this, you'd say that you were most likely in New York.
                            </p>
                            <p>
                                The same idea works for robots when they try to figure out where they are indoors. Say our robot is plopped in a building somewhere with no GPS and no idea where it is, but it is given a floor plan of the building. Using this floor plan, the robot can calculate how likely it is to see a particular intersection or hallway or other landmark from any given location in the building: this gives us our sensor model. Then the robot can generate a number of guesses at possible locations scattered around the building, and calculate how likely it is to see its current view from each of those possible locations. By moving and updating probabilities as shown below, the robot's guesses eventually reduce to one near the true position.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/MCL_init.png">
                                </div>
                                <div>
                                    <div>
                                        To navigate with vision, first we populate the map with guesses, then calculate how likely we are to see what we're actually seeing for each guess.
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/MCL_resampling.png">
                                </div>
                                <div>
                                    <div>
                                        We then generate new guesses from the old ones according to their likelihood; only the guesses seeing something similar to the actual image are chosen.
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/MCL_motion.png">
                                </div>
                                <div>
                                    <div>
                                        Next we move the robot, and based on our knowledge of how the robot moved we move our guesses as well, and recalculate the sensing probabilities.
                                    </div>
                                </div>
                            </div>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/MCL_final.png">
                                </div>
                                <div>
                                    <div>
                                        As the robot moves, it becomes less likely that guesses far from the true position see the same thing as the robot, so all the guesses converge to the true location.
                                    </div>
                                </div>
                            </div>
                            <p>
                                This algorithm is called Monte Carlo Localization, and it's been used successfully on a number of robots. By using probabilities, we can compare how "close" a guess location's view is to the true view. There are an infinite number of possible robot locations, and if we had to guess the exact right location we'd never get it.
                            </p>
                            <p>
                                Vision-based localization is also useful for more than just robots. Using the same principles, a number of scientists and researchers are designing programs that will let you use your cell phone camera to take pictures of a street corner and tell you where you are.
                            </p>
                            <h3>
                                Recognizing the Environment
                            </h3>
                            <p>
                                To move around and achieve its designed missions, a robot not only needs to know where it is, but it also needs to recognize and identify what it encounters in its environment. At the most basic level, the machine has to know where it can move and where it can't. For the autonomous cars crossing the desert in the DARPA Grand Challenge, this means distinguishing passable terrain from rocks and other obstacles. Robots in more complicated settings, like city driving or a crowded office building, may need to do even better, and recognize and distinguish between specific objects.
                            </p>
                            <h4>
                                Terrain Classification
                            </h4>
                            <p>
                                In order to identify where it was safe to drive, cars in the DARPA Grand Challenge desert races ran algorithms to classify the terrain. Most used a scanning laser rangefinder (LIDAR, a kind of laser radar) to identify road surfaces and spot impassible areas by measuring the height of the terrain. If an area of ground has many sudden, sharp changes in height, then it's unwise to drive over it.
                            </p>
                            <p>
                                The LIDAR is bolted to the car, and in order to accurately measure the height of obstacles you detect you need to know where the car is pointed. As the car bounces and skids over rough terrain there may be errors in the orientation estimate. At longer distances, an error in angle of a few degrees is enough to produce a large error in height, potentially causing us to miss obstacles until they're too close or creating spurious obstacles that aren't there.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/obstacle_error.png">
                                </div>
                                <div>
                                    <div>
                                        A small error in the vehicle's orientation can lead to a large errors when calculating terrain height from laser scans.
                                    </div>
                                </div>
                            </div>
                            <p>
                                However, even though we don't know what the exact error in our orientation is, we can characterize it and get the probability over possible errors: how likely are we to be close to the true angle? This lets us create a sensor model for relating the true height to the measured height through the angle error. Now we're on familiar ground. We can use Bayesian updates to combine multiple measurements and update our belief for the terrain height in the same way we did in finding our robot's position, greatly reducing the effect of errors. We can also combine the scans of different sensors, for example using a camera that can see shadows to tell us where to expect changes in height.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/obstacle_estimate.png">
                                </div>
                                <div>
                                    <div>
                                        By using a probabilistic model of our laser's orientation errors and using Bayes' Rule we can combine multiple readings to generate a best guess.
                                    </div>
                                </div>
                            </div>
                            <h4>
                                Object Recognition
                            </h4>
                            <p>
                                In the desert, with no one else around, it's safe enough to make a binary classification of "passable" versus "impassible." Other situations require a more fine-grained level of recognition. For a car in a populated area, moving obstacles like people and other cars need to be distinguished from static obstacles like cacti. You may even want to be able to identify different kinds of cars, since a big truck will move differently than a sports car.
                            </p>
                            <p>
                                For recognition, the underlying state we're trying to estimate isn't a position in space, but rather the category or identity of an object. We have a set of labels that we can assign to what we see, for example "cactus" and "person," and the state we're trying to estimate is the correct label to assign. The way a robot would approach this problem is similar to how a person might: identify a set of features that can be used to distinguish the labels, and look for those features to make a decision. The features can be general traits like color, size, and shape, or more specific things like a logo on a certain company's cars.
                            </p>
                            <p>
                                In order to associate these features with our labels, we use our friend Bayes' Rule. Our sensor measurements are the features we see, so our sensor probabilities need to tell us how likely we are to see a feature for a given object. Then we can update our belief about which is the correct label using Bayes' Rule, and once the probability for a label crosses a pre-defined threshold we can declare that to be the right label.
                            </p>
                            <p>
                                Using probability in recognition gives us the same advantages as in navigation: we can mix and match features from many different sensors, and we're more robust to conflicting sensor measurements because we know how to weigh different measurements against each other. In the recognition case, we can also use the prior probability term to tell the robot what to expect: for example, we know that in general we're less likely to see cacti in the city, so when our robot leaves the desert we can decrease the prior on the "cacti" label. This allows the robot to come to a conclusion more quickly about what it's seeing, since it discounts the "cactus" option.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/object_recognition.png">
                                </div>
                                <div>
                                    <div>
                                        To recognize objects, we can perform a Bayesian update on our object label probabilities using a model that gives us the probability of seeing particular features.
                                    </div>
                                </div>
                            </div>
                            <p>
                                The tricky part in object recognition isn't figuring out the posterior probability (our belief after a sensor measurement). Bayes' Rule gives that to us. The hard part is figuring out the sensor probabilities: if an object is a cactus, how likely are we to see green? If it's a person? We can answer these questions using machine learning.
                            </p>
                            <p>
                                To figure out what the feature probabilities are, we can show the robot's recognition algorithm many different examples of objects from a category: lots of pictures of different people and cacti from different angles and different lighting conditions. The details of how researchers implement learning algorithms can (and do) fill many textbooks, but the basic idea is that the algorithm can march through the learning examples and count the number of times various features appear associated with the object type of interest. This then gives you the probability that a given feature will be found for each label.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/object_learning.png">
                                </div>
                                <div>
                                    <div>
                                        The probabilities of seeing any given feature when observing a particular object type can be learned from observing a large amount of data.
                                    </div>
                                </div>
                            </div>
                            <p>
                                One great demonstration of this process is a contest held every year called the Semantic Robot Vision Challenge. In this contest, robots are given a list (in words) of objects they've never seen before, and they have to drive around a contest area trying to find those objects using vision. So how do they figure out what those objects look like? Same way we answer pretty much all questions these days: Google.
                            </p>
                            <p>
                                The robots go on Google Image Search and key in the names of the objects they're assigned to find. Enter "stapler," for example, and hundreds of pictures of staplers are returned, in all shapes and sizes and orientations. The robots can sort through these pictures, sifting out features to look for and learning their associated probabilities. Then they set out on to the contest floor to find the objects, and the robots are scored on accuracy in finding the objects.
                            </p>
                            <p>
                                The advantage of doing recognition and learning using probability is that it lets us use massive, noisy datasets like Google to learn automatically. Errors and patterns in the data don't have to be identified by hand; instead they're naturally counted and assigned probabilities according to how often they appear. Once we have the feature probabilities we just plug our measurements into Bayes' Rule, turn the crank and out pop our label probabilities. Given enough data, we can learn and classify the whole world, a goal that Google and many computer scientists are pursuing.
                            </p>
                            <h3>
                                Decision-Making and Control
                            </h3>
                            <p>
                                Now that our robot knows where it is and what's around it, how do we use this information? There are about as many ways to do this as there are robots, and the specifics vary greatly depending on the application the robot is designed for. However, there are some generalizations that we can make.
                            </p>
                            <p>
                                First, there's a distinction in how robots use their state estimates. In most cases, a robot won't use the full probability distribution over possible states for control; instead they pick an average or most-likely state, assume it's true, and base their control on that. This works well in situations where the most likely states are clustered close together, for example your estimate of the robot's position will most often be in a single area. The alternative is to consider all of the possible states, which can be much more difficult.
                            </p>
                            <p>
                                Second, robot control algorithms can be broken down into two categories: reactive and planning. Reactive algorithms are like your reflexes: they react instantaneously to the current situation in a precisely defined way. You can also think of a reactive algorithm as being like a set of if statements: if A happens, then do B.
                            </p>
                            <p>
                                A good example of this kind of control is the Roomba vacuum cleaner. The Roomba has a set of behaviors that tell it to back up or turn or spin when sensors like its bumper are triggered or certain time limits are reached. Usually the robot's creators design reactive algorithms offline before the robot is deployed; iRobot (Roomba's creators) spent a lot of time and money figuring out a good set of basic behaviors for the Roomba. Without any idea of what your room looks like, the Roomba can cover most of it given enough time to run through its patterns, because the patterns are designed to cover an area given enough time.
                            </p>
                            <p>
                                Planning algorithms are more like your higher brain functions. They look ahead into the future, selecting the best possible action from the different choices the robot has at the present time. These algorithms generate predictions of the possible results of each action using the robot's internal models and estimate of the state. Each result is then evaluated relative to the others using some metric, usually supplied ahead of time by the robot's creators, and the best action is chosen.
                            </p>
                            <p>
                                Although there are many simple robots that work purely on a reactive level, many use both. The Grand Challenge's autonomous cars provide us with a good example of this. Most of the vehicles ran with two levels of control software. A high-level planner would calculate a desired trajectory through passable terrain ahead to the next GPS waypoint, trying to pick a route that minimizes travel time. This trajectory is then passed to a low-level reactive controller that commands steering angles to keep the vehicle on the desired path.
                            </p>
                            <div>
                                <div>
                                    <img src="http://static.arstechnica.com/robot-thinking/DGC_vehicle_structure.png">
                                </div>
                                <div>
                                    <div>
                                        A typical sensing-planning-control architecture for a DARPA Grand Challenge-type robotic car.
                                    </div>
                                </div>
                            </div>
                            <p>
                                The alternative to assuming a single true state is harder: the robot has to consider all the possible outcomes. This can eat up a lot of computation time. Take a simple example with only two possible state values: a planning algorithm that considers the possibility that a patch of ground it wants to go through is either passable or impassible. Passable terrain has a low cost, and impassible terrain has a very high cost since you really don't want to drive over it.
                            </p>
                            <p>
                                Since impassible terrain has a very high cost, we only drive over a bit of ground if we're really sure that it's passable, which is why the robots tend to be conservative, and rightly so, with respect to obstacles. For example, say there's a bit of ground in front of us that we think is either a large rock or a small bush. If we think there's a fifty-fifty chance that it's a rock and we decide to put our path through it, then there's a fifty percent chance that we'll get stuck and end the race right there versus a fifty percent chance that it's a bush and we'll save about 30 seconds. On the other hand, if we go around it there's a 100 percent chance that we lose 30 seconds. Faced with that choice, it's pretty clear that the right thing is to choose a path that goes around.
                            </p>
                            <p>
                                The example with only one obstacle to consider is straightforward, but now say there's two obstacles. That means there are 4 possible states: both there, one or the other there, or both absent. As you increase the number of state variables, there are an exponentially growing number of possible values to consider. Considering all possibilities makes sure you find the best possible answer, but you might not get to it in a reasonable time. Some of the most interesting current research goes even beyond this. Instead of considering only the possible end states after a robot action, we can also consider the possible beliefs a robot has about its state after an action. This kind of meta-control can lead to much smarter robots, robots that know when they don't know and that take active action to improve their understanding. This leads to even bigger computational headaches. Figuring out how to solve such problems in a reasonable amount of time is a hot area of current robotics research.
                            </p>
                            <h3>
                                Conclusion
                            </h3>
                            <p>
                                Hopefully, we've managed to convey the importance of sensing in robotics and the important role that probability plays in sensing. A lot of hard work by a lot of people has been condensed in to a few short pages, and much has been left out, so apologies for glossing over many details. Looking at an overview like this, it always seems like the tasks that are being described are so easy: just take the features and calculate the probabilities and hey, presto, we've "solved" object recognition!
                            </p>
                            <p>
                                Of course, the devil is in the details; the algorithms we've talked about are all computational gluttons. By introducing probability and calculating over possibilities, the robots are more capable of dealing with uncertainty, but it also means that you have to run a lot of computations. Probabilistic robotics has made the modern robots that we see possible, and Moore's Law has helped make probabilistic robotics feasible, but there's still a lot of work left before we have real thinking machines.
                            </p>
                            <p>
                                We'll leave off with some intriguing news from another discipline. In recent years, the fields of robotics and neuroscience have often found common ground. Scientists are discovering that in many things, humans work the same way we've come to make our robots work. Remember object recognition? Researchers have done experiments showing that there are specific portions of our visual cortexes that light up in response to particular features. There are neurons that light up specifically for things like eyes, noses, and ears. The eyes and nose neurons feed up to neurons that recognize faces, providing features for face recognition just like on a robot.
                            </p>
                            <p>
                                The really interesting thing is that there have been experiments showing that humans, when asked to perform certain visual perception, language processing, and motor control tasks, produce results identical to software using Bayes' Rule with probabilistic models. It's too early to say for sure, but might the human brain be a giant neural calculator, running Bayes' Rule on probability distributions that we've been learning all our lives? Who knowsâ€”we might be closer to thinking machines than we thought.
                            </p>
                            <p>
                                <a href="http://arstechnica.com/science/news/2010/03/how-robots-think.ars?comments=1&amp;utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss#comments-bar">Read the comments on this post</a>
                            </p>
                        </blockquote>
                    </div>
                    <p class="bookmark-source">
                        Source: <a href="http://arstechnica.com/science/news/2010/03/how-robots-think.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss">http://arstechnica.com/science/news/2010/03/how-robots-think.ars?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</a>
                    </p>
                </div>
            </article>
            <nav id="post-nav"></nav><script id="discus-javascript">
    var disqus_shortname = 'improbable';

                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                    document.getElementsByTagName('body')[0].appendChild(dsq);
                })();
            </script><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
            <div id="disqus_thread"></div>
        </section>
        <footer id="site-footer" class="nocontent">
            <p>
                This site is purely my personal work and does not reflect the views of my employer.
            </p>
            <p class="license">
                <a class="icon" rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png"></a> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
            </p>
        </footer><script async="" defer src="/static/js/common.js">
</script><script id="google-analytics">
    var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-2097834-1']);
            _gaq.push(['_setDomainName', 'improbable.org']);
            _gaq.push(['_trackPageview']);

            (function() {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
        </script>
    </body>
</html>
