<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>
            New Crawlers At Technorati
        </title>
        <meta name="viewport" content="width=device-width,initial-scale=1.0">
        <link rel="stylesheet" href="/static/css/main.min.css" type="text/css" media="all"><!--[if lte IE 8]>
            <link rel="stylesheet" href="/static/css/ie-fixes.css" type="text/css" media="all">
            <script src="/static/js/html5shiv.js"></script>
        <![endif]-->
        <meta http-equiv="last-modified" content="Thu, 05 Mar 2009 04:31:16 GMT">
        <link rel="alternate" type="application/atom+xml" title="All Posts (Atom)" href="/feeds/all.atom">
        <link rel="alternate" type="application/rss+xml" title="All Posts (RSS)" href="/feeds/all.rss">
        <link rel="icon" type="image/jpeg" sizes="287x287" href="/static/img/favicon-287x287.jpg">
        <link rel="icon" type="image/jpeg" sizes="128x128" href="/static/img/favicon-128x128.jpg">
        <script type="application/javascript">
var _prum={id:"5166be01e6e53d1007000001"};var PRUM_EPISODES=PRUM_EPISODES||{};PRUM_EPISODES.q=[];PRUM_EPISODES.mark=function(b,a){PRUM_EPISODES.q.push(["mark",b,a||new Date().getTime()])};PRUM_EPISODES.measure=function(b,a,b){PRUM_EPISODES.q.push(["measure",b,a,b||new Date().getTime()])};PRUM_EPISODES.done=function(a){PRUM_EPISODES.q.push(["done",a])};PRUM_EPISODES.mark("firstbyte");(function(){var b=document.getElementsByTagName("script")[0];var a=document.createElement("script");a.type="text/javascript";a.async=true;a.charset="UTF-8";a.src="//rum-static.pingdom.net/prum.min.js";b.parentNode.insertBefore(a,b)})();
        </script>
    </head>
    <body class="blog post_detail" itemscope itemtype="http://schema.org/BlogPosting">
        <nav id="site-nav">
            <header id="about">
                <h1>
                    <a href="/about/">Chris Adams</a>
                </h1>
                <h2>
                    Programmer, cyclist, photographer
                </h2>
            </header>
            <ul class="links">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/about/">About</a>
                </li>
                <li>
                    <a href="/feeds/all.atom" title="Site Feed">Site Feed</a>
                </li>
            </ul>
            <ul class="social-networks">
                <li>
                    <a href="https://github.com/acdha" rel="me">Github</a>
                </li>
                <li>
                    <a href="http://bitbucket.org/acdha" rel="me">Bitbucket</a>
                </li>
                <li>
                    <a href="http://delicious.com/acdha" rel="me">del.icio.us</a>
                </li>
                <li>
                    <a href="http://twitter.com/acdha" rel="me">Twitter</a>
                </li>
                <li>
                    <a href="https://plus.google.com/116562742092842686896?rel=author" rel="me">Google+</a>
                </li>
                <li>
                    <a href="http://www.flickr.com/photos/acdha" rel="me">Flickr</a>
                </li>
                <li>
                    <a href="http://www.linkedin.com/in/acdha" rel="me">LinkedIn</a>
                </li>
                <li>
                    <a href="http://connect.garmin.com/explore?owner=acdha" rel="me">Garmin Connect</a>
                </li>
            </ul>
            <div id="site-search"></div>
        </nav>
        <section id="main">
            <article class="post">
                <header>
                    <meta itemprop="dateCreated" content="2009-03-04T23:31:16-04:00">
                    <meta itemprop="dateModified" content="2009-03-04T23:31:16-04:00"><time class="date" itemprop="datePublished" datetime="2009-03-05T03:31:16+00:00">Mar 05</time>
                    <h2 itemprop="title">
                        New Crawlers At Technorati
                    </h2>
                </header>
                <div class="body" itemprop="articleBody">
                    <div class="googlereader description" data-google-id="tag:google.com,2005:reader/item/6ef4c61efb52b0cd">
                        <blockquote>
                            <p>
                                <a href="http://www.flickr.com/photos/kitchenmage/1395622544/" title="Spider Web"><img src="http://www.arachna.com/images/spiderweb.jpg" style="float:right;margin:5px 5px 5px 5px" border="0" width="128" height="134"></a> A lot of changes are afoot at Technorati. Over the last year or so, we've been looking inward at the infrastructure and asking ourselves, "How can we do this better?". The data spigot that Technorati builds on was the first thing to focus on, it's a critical part in one leg of the back-end infrastructure tripod. The tripod consists of data acquisition, search and analytics Technorati; while the ping handling and queuing are relatively simple affairs the crawler is the most sophisticated of the data acquisition subsystems. It's proper functioning is critical to the functioning of the other legs; when it doesn't function well, search and analytics don't either (GIGO="garbage in/garbage out").
                            </p>
                            <p>
                                As <a href="http://dorion.blogspot.com/">Dorion</a> mentioned recently, <a href="http://technorati.com/weblog/2009/02/473.html">we're retiring the old crawler</a>. Why are we giving the old crawler getting an engraved watch and showing it to the door? Well, old age is one reason. The original spider is a technology that dates back to 2003, the blogosphere has changed a lot since then and we have a much better developed understanding of the requirements. The original spider code has presented a sufficient number of GIGO-related and code maintenance challenges to warrant a complete re-thinking. It contrasts starkly with the replacement.
                            </p>
                            <dl>
                                <dt>
                                    Data model
                                </dt>
                                <dd>
                                    There are a lot of ways to derive structural information out of the pages and feeds that a blog presents. The old spider used event driven parses, building a complex state as it went with flat data structures (lists and hashes). The new one uses the composed web documents to populate a well-defined object model; all crawls normalize the semi-structured data found on the web to that model.
                                </dd>
                                <dt>
                                    Crawl persistence
                                </dt>
                                <dd>
                                    The old spider was hard-wired to persist the aforementioned data structure elements to relational databases (sharded MySQL instances) while it was parsing, so that the flow of saving parsed data was closely coupled with parsing events, forsaking transactional integrity and consuming costly resources. The new spider composes and saves its parse result as a big discreet object (not collections of little objects in an RDBMS). This reduced the hardware footprint by an order of magnitude.
                                </dd>
                                <dt>
                                    Operational visibility
                                </dt>
                                <dd>
                                    Whether a blog's page structure was understood (or not), the feed was well formed (or not) or any of the many other things that determine the success or quality of a blog's crawl was opaque under the old spider. With the new spider, detailed metadata and metrics are tracked during the crawl cycles. This better enables the team to support bloggers and extend the system's capabilities.
                                </dd>
                                <dt>
                                    Unit tests
                                </dt>
                                <dd>
                                    Wherever you have complex, critical software you want to have unit tests. The old spider had almost no unit tests and was developed in a way that made testing the things that mattered most exceptionally difficult. The new spider was developed with a test harness upfront, it now has hundreds of tests that validate thousands of aspects of the code. The test are uniformly invoked by the developers and automatically whenever the code is updated (AKA under <a href="http://martinfowler.com/articles/continuousIntegration.html">continuous integration</a>).
                                </dd>
                            </dl>The old spider didn't leverage packages to logically separate the different concerns (fetching, parsing, validation, change determination, etc), the aforementioned flat data structures, mingling of concerns and absence of unit tests made changing it exceedingly difficult. Now, we have a whole that is greater than the sum of the parts; having a well defined data model, sensible persistence, operational visibility and unit tests has added up to an order of magnitude improvement across several dimensions. The real benefits are seen when we've shown that the system is easy to change, I mentioned this several weeks ago when I noted the ease with which we could <a href="http://www.arachna.com/roller/spidaman/entry/inbloguration_one_week_later" title="Inbloguration: One Week Later">adapt custom requirements to crawl the White House blog</a>.
                            <p>
                                Another change that we've made is to the legacy assumption that everything that pings is a blog. That assumption proved to be increasingly untenable as the ping meme spread amongst those who didn't really understand the difference between some random page and a blog, nefarious publishers (spammers) and other perpetrators of <a href="http://en.wikipedia.org/wiki/Sping">spings</a>. Over 90% of the pings hitting Technorati are rejected outright because they've been identified as invalid pings. A large portion of the remainder are later determined to be invalid but we now have a rigorous system in place for filtering out the noise. We've reduced the spam level considerably (as mentioned in a <a href="http://www.arachna.com/roller/spidaman/entry/social_media_backlash_against_cheaters" title="Social Media Backlash Against Cheaters and Fleshmongers">prior post</a>). For instance, there's a whole genre of <a href="http://en.wikipedia.org/wiki/Spam_blog">splogs</a> that are pornography focused (hardcore pictures, paid affiliate links, etc) that previously plagued our data; now we've eliminated a lot of that nonsense from the index.
                            </p>
                            <p>
                                Here are a pair of charts showing the daily occurrence of a particular porn term in the index.
                            </p>
                            <div style="float:left;width:48%;height:220px">
                                6 month retrospective as of November 3rd, 2008:<br>
                                <img src="http://www.arachna.com/images/blowjob_180_day_20081103.png" width="300" height="180" style="margin:5px 5px 5px 5px;vertical-align:bottom" border="0">
                            </div>
                            <div style="float:right;width:48%;height:220px">
                                6 month retrospective today, 5 months later:<br>
                                <img src="http://www.arachna.com/images/blowjob_180_day_20090303.png" width="300" height="180" style="margin:5px 5px 5px 5px" border="0">
                            </div>
                            <p style="clear:both">
                                As you can see, that's an order of magnitude reduction; 90% of the occurrences of that term was spam.
                            </p>
                            <p style="clear:both">
                                So what's next for the crawler? We've got some stragglers on the old spider, we're going to migrate them over in the next few days. There are still a lot of issues to shake out, as with any new software (for instance, there are still some error recovery scenarios to deal with). But it's getting better all of the time (love that song). We'll be rolling out new tools internally for identifying where improvements are needed, ultimately we'd like to enable bloggers to help themselves to publish, get crawled, be found and recognized more effectively. And there are more changes afoot, stay tuned.
                            </p>
                            <p>
                                <em><a href="http://technorati.com/tag/technorati" rel="tag">technorati</a> &nbsp; <a href="http://technorati.com/tag/web+crawling" rel="tag">web crawling</a> &nbsp; <a href="http://technorati.com/tag/software" rel="tag">software</a> &nbsp; <a href="http://technorati.com/tag/spam" rel="tag">spam</a> &nbsp; <a href="http://technorati.com/tag/splogs" rel="tag">splogs</a> &nbsp;</em>
                            </p>
                        </blockquote>
                    </div>
                    <p class="bookmark-source">
                        Source: <a href="http://www.arachna.com/roller/spidaman/entry/new_crawlers_at_technorati">http://www.arachna.com/roller/spidaman/entry/new_crawlers_at_technorati</a>
                    </p>
                </div>
            </article>
            <nav id="post-nav"></nav><script id="discus-javascript">
    var disqus_shortname = 'improbable';

                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                    document.getElementsByTagName('body')[0].appendChild(dsq);
                })();
            </script><a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
            <div id="disqus_thread"></div>
        </section>
        <footer id="site-footer" class="nocontent">
            <p>
                This site is purely my personal work and does not reflect the views of my employer.
            </p>
            <p class="license">
                <a class="icon" rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png"></a> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.
            </p>
        </footer><script async="" defer src="/static/js/common.js">
</script><script id="google-analytics">
    var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-2097834-1']);
            _gaq.push(['_setDomainName', 'improbable.org']);
            _gaq.push(['_trackPageview']);

            (function() {
                var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
        </script>
    </body>
</html>
